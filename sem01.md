<!--

author:     Alexander Trofimov
attribute:  **Editor:** Daria Gudkova
email:    
date:       21/02/2025
version:    0.1
language:   en
narrator:   English

icon:    ./icon.png
logo:    ./course_logo.png

comment:  Курс "Введение в машинное обучение"

import:   ./macro.md
          https://raw.githubusercontent.com/LiaScript/CodeRunner/master/README.md

link:     ./custom.css

-->


## Семинар 1
-----

!?[Lecture03 Video](https://www.youtube.com/watch?v=8pTEmbeENF4)

В данном семинаре рассматриваются ...

----
<h3 style="text-align: center;">Содержание раздела</h3>

...

### Введение в Scikit-Learn
-----

Существует несколько библиотек Python, которые предоставляют надёжные реализации целого ряда алгоритмов машинного обучения. Одной из самых известных является [Scikit-Learn](http://scikit-learn.org/) — пакет, предоставляющий эффективные версии большого количества распространённых алгоритмов. Scikit-Learn отличается чистым, единообразным и оптимизированным API, а также очень полезной и полной онлайн-документацией. Преимущество такого единообразия заключается в том, что, как только вы освоите базовое использование и синтаксис Scikit-Learn для одного типа моделей, вам будет очень легко перейти к новой модели или алгоритму.

Мы начнём с рассмотрения представления данных в Scikit-Learn, затем перейдём к API Estimator и, наконец, рассмотрим более интересный пример использования этих инструментов для изучения набора изображений рукописных цифр.

----
####  Представление данных в Scikit-Learn
-----

Машинное обучение — это создание моделей на основе данных. Поэтому мы начнём с обсуждения того, как можно представить данные, чтобы компьютер мог их понять. В Scikit-Learn лучше всего рассматривать данные в виде таблиц.

Матрица признаков (Features matrix)
----

**Таблица** — это двумерная сетка данных, в которой строки представляют отдельные элементы набора данных, а столбцы — величины, связанные с каждым из этих элементов. Например, рассмотрим [набор данных Iris](https://en.wikipedia.org/wiki/Iris_flower_data_set). Мы можем загрузить этот набор данных в виде Pandas `DataFrame` с помощью библиотеки [seaborn](http://seaborn.pydata.org/):

<div style="display: none !important; visibility: hidden !important; position: absolute !important; left: -9999px !important;">

```python
import subprocess
import sys

def install_package(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package])

try:
    import sklearn
except ImportError:
    install_package("scikit-learn")
    import sklearn

try:
    import pandas as pd
except ImportError:
    install_package("pandas")
    import pandas as pd

try:
    import seaborn as sns
except ImportError:
    install_package("seaborn")
    import seaborn as sns
```
</div>


``` python
import seaborn as sns
iris = sns.load_dataset('iris')
print(iris.head())
```

Каждая строка данных относится к одному наблюдаемому цветку, а количество строк соответствует общему количеству цветков в наборе данных. Мы будем называть строки матрицы `samples`, а количество строк — `n_samples`.

Аналогичным образом каждый столбец данных относится к определённому количественному показателю (признаку), характеризующему каждый образец. Мы будем называть столбцы матрицы `features`, а количество столбцов — `n_features`.

Из этой таблицы видно, что информацию можно представить в виде двумерного числового массива или матрицы, которую мы будем называть **матрицей признаков** (`features matrix`). По соглашению, эта матрица признаков часто хранится в переменной с именем `X`. Предполагается, что матрица признаков является двумерной, имеет форму `[n_samples, n_features]` и чаще всего содержится в массиве NumPy или Pandas `DataFrame`, хотя некоторые модели Scikit-Learn также принимают разреженные матрицы SciPy.

@img02(iris.png, Iris dataset, 50)

Целевой массив (Target array)
---

Помимо матрицы признаков `X` рассматривают массив меток или целевых значений, который мы будем называть `y`. Целевой массив обычно одномерный, его длина составляет `n_samples`, и он, как правило, содержится в массиве NumPy или Pandas Series. Целевой массив может содержать как непрерывные числовые значения, так и дискретные классы/метки.

Отличительной особенностью целевого массива от других столбцов признаков является то, что это величина, которую мы хотим предсказать на основе данных: говоря статистическим языком, это зависимая переменная. Например, в приведённом выше наборе данных мы можем захотеть построить модель, которая сможет предсказывать вид цветка на основе других измерений; в этом случае столбец `species` будет считаться целевым массивом.

Имея в виду этот целевой массив, мы можем использовать Seaborn для визуализации данных:

``` python
%matplotlib inline
import seaborn as sns; 
sns.set()
sns.pairplot(iris, hue='species', size=1,5)
```

Для использования в Scikit-Learn мы извлечём матрицу признаков и целевой массив из `DataFrame`:

``` python
X_iris = iris.drop('species', axis=1)
X_iris.shape
```

``` python
y_iris = iris['species']
y_iris.shape
```

На рисунке ниже представлен ожидаемый вид матрицы признаков и массива целевых значений.

@img02(feature_target.png, Матрица признаков и массив целевых значений, 50)

Алгоритм использования Scikit-Learn 
----

Чаще всего при использовании Scikit-Learn выполняются следующие действия.

1. Выбрать класс модели, импортировав соответствующий класс оценки из Scikit-Learn. 
2. Выбрать гиперпараметры модели, создав экземпляр этого класса с нужными значениями.
3. Разделить данные на матрицу признаков и целевой массив, как описано выше.
4. Обучить модель на своих данные, вызвав метод fit() экземпляра модели.
5. Применить модель к новым данным:

- При обучении с учителем: предсказать метки для неизвестных данных с помощью метода predict().
- При обучении без учителя: преобразовать данные или определить их свойства с помощью метода transform() или predict().

Теперь рассмотрим несколько простых примеров обучения с учителем и без.

---
#### Обучение с учителем
---

Простая линейная регрессия
---

Рассмотрим простую линейную регрессию, то есть распространённый случай построения прямой по данным (x,y). Сгенерируем простые данные.

``` python
import matplotlib.pyplot as plt
import numpy as np

rng = np.random.RandomState(42)
x = 10 * rng.rand(50)
y = 2 * x - 1 + rng.randn(50)
plt.scatter(x, y)
```

Воспользуемся алгоритмом, описанным ранее.

**1. Выбор класса модели**

В Scikit-Learn каждый класс модели представлен классом Python. Так, например, если мы хотим вычислить простую модель линейной регрессии, мы можем импортировать класс линейной регрессии:

``` python
from sklearn.linear_model import LinearRegression
```

Cуществуют и другие, более общие модели линейной регрессии. Подробнее о них можно прочитать в [документации](http://scikit-learn.org/stable/modules/linear_model.html).

**2. Выбор гиперпараметров модели**

@NB(`Класс модели — это не то же самое, что экземпляр модели.`)

В зависимости от класса модели, с которым мы работаем, нам может потребоваться ответить на один или несколько вопросов, например:

- Подбирать ли значение для свободного члена?
- Хотим ли мы, чтобы модель была нормализована?
- Нужно ли предварительно обработать данные, чтобы повысить гибкость модели?
- Какую степень регуляризации использовать?
- Сколько компонентов модели использовать?

Это примеры важных решений, которые необходимо принять после выбора класса модели. Эти решения часто представлены в виде гиперпараметров или параметров, которые необходимо задать до того, как модель будет обучена на данных. В Scikit-Learn гиперпараметры выбираются путем передачи значений при создании экземпляра модели. Мы рассмотрим, как можно обосновать выбор гиперпараметров [в следующем разделе] (#).

Давайте создадим экземпляр класса `LinearRegression` и укажем, что нам нужно подобрать значение свободного члена с помощью гиперпараметра `fit_intercept`:

``` python
model = LinearRegression(fit_intercept=True)
model
```

При создании экземпляра модели единственным действием является сохранение этих значений гиперпараметров. В частности, мы ещё не применяли модель к каким-либо данным: в API Scikit-Learn чётко разграничены ~~выбор модели~~ и ~~применение модели к данным~~.

**3. Разделение данных на матрицу признаков и целевой массив**

Здесь наша целевая переменная `y` уже имеет правильную форму (массив длиной `n_samples`), но нам нужно обработать данные `x`, чтобы они стали матрицей размером `[n_samples, n_features]`. В данном случае это сводится к простому изменению формы одномерного массива:

``` python
X = x[:, np.newaxis]
X.shape
```

**4. Примение модели к своим данным**

Применить модель к данным можно с помощью метода модели `fit()`:

``` python
model.fit(X, y) 
```

`fit()` запускает ряд внутренних вычислений, зависящих от модели, а результаты этих вычислений сохраняются в атрибутах модели, которые может изучить пользователь. В Scikit-Learn по умолчанию все параметры модели, которые были изучены в ходе fit() процесса, заканчиваются символом подчеркивания; например, в этой линейной модели мы имеем следующее:

``` python
model.coef_, model.intercept_
```

Эти два параметра представляют собой ~~наклон~~ и ~~точку пересечения~~ с осью ординат простой линейной аппроксимации данных (свободный член уравнения регрессии). Сравнивая со сгенерированными в начале данными, можно заметить, что они очень близки к исходному наклону, равному 2, и точке пересечения с осью ординат, равной -1.

**5. Прогнозирование меток для неизвестных данных**

После обучения модели основная задача обучения с учителем состоит в оценке её предсказаний на новых данных, которые не входили в обучающую выборку. В Scikit-Learn это можно сделать с помощью метода `predict()`. В этом примере нашими «новыми данными» будет сетка из значений `x`, и мы спросим, какие значения `y` предсказывает модель:

``` python
xfit = np.linspace(-1, 11)
```

Как и раньше, нам нужно преобразовать эти значения `x` в `[n_samples, n_features]` матрицу признаков, после чего мы сможем передать её в модель:

``` python
Xfit = xfit[:, np.newaxis]
yfit = model.predict(Xfit)
```

Давайте визуализируем результаты, построив график сначала для необработанных данных, а затем для полученной модели:

``` python
plt.scatter(x, y)
plt.plot(xfit, yfit)
```

Классификация
---

Рассмотрим задачу классификации данных на примере набора данных Iris. Наш вопрос будет таким: насколько хорошо модель, обученная на части данных Iris, сможет предсказать остальные метки? 

Для решения этой задачи будем использовать наивный байесовский классификатор Гаусса, который предполагает, что каждый класс соответствует распределению Гаусса с осями, выровненными по центру. Поскольку наивный байесовский классификатор Гаусса работает очень быстро и не требует выбора гиперпараметров, его часто используют в качестве базовой модели для классификации, прежде чем искать улучшения с помощью более сложных моделей.

Мы хотели бы оценить модель на данных, с которыми она раньше не сталкивалась, поэтому мы разделим данные на обучающую  и тестовую выборки. Это можно сделать вручную, но удобнее использовать вспомогательную функцию `train_test_split`:

``` python
from sklearn.cross_validation import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris, random_state=1)
```

Теперь можем воспользоваться алгоритмом для прогнозирования меток: 

``` python
from sklearn.naive_bayes import GaussianNB # 1. выбор класса модели
model = GaussianNB() # 2. создание экземпляра модели
model.fit(Xtrain, ytrain) # 3. обучение модели на данных
y_model = model.predict(Xtest) # 4. прогнозирование меток для новых данных
```

Наконец, мы можем использовать утилиту `accuracy_score`, чтобы узнать, какая доля предсказанных меток соответствует их истинному значению: 

``` python
from sklearn.metrics import accuracy_score
accuracy_score(ytest, y_model)
```

Точность более $97\% $показывает, что даже этот очень простой алгоритм классификации эффективен для данного набора данных!

#### Обучение без учителя
----

Уменьшение размерности данных 
----

В качестве примера задачи обучения без учителя рассмотрим уменьшение размерности данных Iris, чтобы их было проще визуализировать. Данные Iris являются четырёхмерными: для каждого образца записаны четыре признака.

Задача уменьшения размерности состоит в том, чтобы найти подходящее представление с меньшим количеством измерений, которое сохранит основные характеристики данных. Часто уменьшение размерности используется для визуализации данных: гораздо проще отобразить данные в двух измерениях, чем в четырёх или более!
Здесь мы будем использовать метод главных компонент (МГК; см. Подробнее: [метод главных компонент ](https://code.vt.edu/cci-ai-testbed/python-data-science-handbook/-/blob/cci/notebooks/05.09-Principal-Component-Analysis.ipynb)), который представляет собой быстрый метод линейного уменьшения размерности. Мы попросим модель вернуть две компоненты, то есть двумерное представление данных.
Следуя последовательности шагов, описанной выше, получаем: 

``` python
from sklearn.decomposition import PCA # 1. Выбор класса модели
model = PCA(n_components=2) # 2. Создание экземпляра модели с гиперпараметрами
model.fit(X_iris) # 3. Обучение по данным. Обратите внимание, что y не указано!
X_2D = model.transform(X_iris) # 4. Преобразование данных в двумерные 
```

Теперь давайте визуализируем результат. Один из способов сделать это — вставить результаты в исходный код Iris DataFrame и использовать `lmplot`: 

``` python
iris['PCA1'] = X_2D[:, 0] 
iris['PCA2'] = X_2D[:, 1] 
sns.lmplot("PCA1", "PCA2", hue='species', data=iris, fit_reg=False)
```

Можно заметить, что в двумерном пространстве виды `species` довольно хорошо разделимы, хотя алгоритм главных компонент не знал, какие виды представлены! Это говорит о том, что относительно простая классификация, вероятно, будет эффективной для данного набора данных.

Кластеризация 
----

Рассмотрим задачу кластеризации на примере датасета Iris. Алгоритм кластеризации пытается найти отдельные группы данных без привязки к каким-либо меткам. 

Для примера в качестве метода кластеризации будем использовать Gaussian mixture model (GMM), подробное описание которого приведено можно найти [здесь](https://code.vt.edu/cci-ai-testbed/python-data-science-handbook/-/blob/cci/notebooks/05.12-Gaussian-Mixtures.ipynb).

``` python
from sklearn.mixture import GMM # 1. Выбор класса модели
model = GMM(n_components=3, covariance_type='full') # 2. Инициализация модели гиперпараметрами
model.fit(X_iris) # 3. Обучение на данных. Обратите внимание, что y не указан!
y_gmm = model.predict(X_iris) # 4. Определение меток кластеров
```

Как и раньше, добавим метку кластера в Iris `DataFrame` и используем Seaborn для построения графика: 

``` python
iris['cluster'] = y_gmm 
sns.lmplot("PCA1", "PCA2", data=iris, hue='species',  col='cluster', fit_reg=False)
```

Разделив данные по номерам кластеров, мы видим, насколько хорошо алгоритм GMM восстановил исходную метку: вид setosa полностью отделен в кластер 0, в то время как между versicolor и virginica остается небольшое смешивание. Это означает, что даже без помощи эксперта, который мог бы определить вид отдельных цветов, измерения этих цветов достаточно различны, чтобы мы могли автоматически определить наличие различных видов с помощью простого алгоритма кластеризации!

#### Идентификация рукописных цифр
----

Чтобы продемонстрировать эти принципы на более интересной задаче, рассмотрим задачу идентификации рукописных цифр. В реальных условиях эта задача включает в себя как поиск, так и идентификацию символов на изображении. Здесь мы сократим путь и воспользуемся набором предварительно отформатированных цифр из Scikit-Learn.

Загрузка и визуализация данных о цифрах
----

Загрузим датасет предварительно отформатированных рукописных цифр из Scikit-Learn и посмотрим на размер данных:

``` python
from sklearn.datasets import load_digits
digits = load_digits()
digits.images.shape
```

Данные представляют собой трёхмерный массив: $1797$ примеров, каждый из которых состоит из сетки пикселей размером $8 × 8$.  Визуализируем первые сто из них:

``` python
import matplotlib.pyplot as plt

fig, axes = plt.subplots(10, 10, figsize=(8, 8),
            subplot_kw={'xticks':[], 'yticks':[]},
            gridspec_kw=dict(hspace=0.1, wspace=0.1))

for i, ax in enumerate(axes.flat):
    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')
    ax.text(0.05, 0.05, str(digits.target[i]),
    transform=ax.transAxes, color='green')
```

Чтобы работать с этими данными в Scikit-Learn, нам нужно двумерное представление `[n_samples, n_features]`. Рассмотрим каждый пиксель на изображении как признак: каждая цифра будет представлена массивом значений пикселей длиной $64$. Кроме того, нам нужен целевой массив, который дает ранее определенную метку для каждой цифры. Эти две величины встроены в набор данных `digits` под атрибутами `data` и `target` соответственно:

``` python
X = digits.data
X.shape
```

``` python
y = digits.target
y.shape
```

В датасете содержится $1797$ примеров и $64$ признака.

Обучение без учителя: уменьшение размерности
----

Невозможно эффективно визуализировать данные в $64$-мерном пространстве, поэтому уменьшим размерность до $2$, используя unsupervised learning. В качестве алгоритма уменьшения размерности будем использовать [Isomap](https://code.vt.edu/cci-ai-testbed/python-data-science-handbook/-/blob/cci/notebooks/05.10-Manifold-Learning.ipynb):

``` python
from sklearn.manifold import Isomap
iso = Isomap(n_components=2)
iso.fit(digits.data)
data_projected = iso.transform(digits.data)
data_projected.shape
```

Построим график этих двумерных данных, чтобы понять, сможем ли мы что-нибудь узнать из их структуры:

``` python
plt.scatter(data_projected[:, 0], data_projected[:, 1], c=digits.target, edgecolor='none', alpha=0.5, cmap=plt.cm.get_cmap('spectral', 10))
plt.colorbar(label='digit label', ticks=range(10))
plt.clim(-0.5, 9.5)
```

Этот график дает представление о том, насколько хорошо рукописные цифры разделены в $64$-мерном пространстве. Например, нули (выделенные черным цветом) и единицы (выделенные фиолетовым цветом) практически не пересекаются в пространстве параметров. Интуитивно понятно почему это так: изображение нуля в середине пусто, в то время как единица обычно имеет чернила посередине. С другой стороны, кажется, что существует более или менее непрерывный спектр между единицами и четверками: некоторые люди рисуют единицы с "шляпками" на них, из-за чего они выглядят похожими на четверки.

В целом, различные группы довольно хорошо разделены в пространстве параметров: это говорит о том, что даже очень простой алгоритм классификации должен работать с этими данными надлежащим образом.

Обучение с учителем: классификация рукописных цифр
----

Применим алгоритм классификации. Как и в случае с данными Iris ранее, разделим данные на обучающий и тестовый наборы и применим гауссовскую наивную байесовскую модель:

``` python
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)

from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
model.fit(Xtrain, ytrain)
y_model = model.predict(Xtest)
```

Теперь оценим точность модели, сравнив истинные значения меток тестового набора с предсказанными:

``` python
from sklearn.metrics import accuracy_score
accuracy_score(ytest, y_model)
```

Даже с помощью этой простой модели мы получаем точность классификации около $80\%$! Однако это единственное число не говорит нам о том, где мы ошиблись. Один из способов определить это -- использовать матрицу ошибок:

``` python
from sklearn.metrics import confusion_matrix

mat = confusion_matrix(ytest, y_model)

sns.heatmap(mat, square=True, annot=True, cbar=False)
plt.xlabel('predicted value')
plt.ylabel('true value')
```

Матрица ошибок показывает, где находятся точки с неправильными метками: например, в данном случае большое количество двоек неправильно классифицируются как единицы или восьмерки. Еще один способ получить представление о характеристиках модели -- снова нанести на график входные данные с их предсказанными метками. Будем использовать зеленый цвет для правильных надписей и красный -- для неправильных:

``` python
fig, axes = plt.subplots(10, 10, figsize=(8, 8), subplot_kw={'xticks':[], 'yticks':[]}, gridspec_kw=dict(hspace=0.1, wspace=0.1))

test_images = Xtest.reshape(-1, 8, 8)

for i, ax in enumerate(axes.flat):
    ax.imshow(test_images[i], cmap='binary', interpolation='nearest')
    ax.text(0.05, 0.05, str(y_model[i]),
    transform=ax.transAxes,
    color='green' if (ytest[i] == y_model[i]) else 'red')
```

Чтобы выйти за рамки $80$-процентного показателя классификации, мы могли бы перейти к более сложному алгоритму, например:

- [метод опорных векторов](https://code.vt.edu/cci-ai-testbed/python-data-science-handbook/-/blob/cci/notebooks/05.07-Support-Vector-Machines.ipynb) 
- [решающее дерево и случайный лес](https://code.vt.edu/cci-ai-testbed/python-data-science-handbook/-/blob/cci/notebooks/05.08-Random-Forests.ipynb).

### Гиперпараметры и валидация модели
----

Чтобы сделать осознанный выбор модели и гиперпараметров, нужно проверить, насколько хорошо наша модель и гиперпараметры соответствуют данным.

Кажется, что проверка модели очень проста: после выбора модели и её гиперпараметров мы можем оценить её эффективность, применив её к части обучающих данных и сравнив прогноз с известным значением. В следующих разделах сначала рассматривается наивный подход к проверке моделей и объясняется, почему он не работает, а затем изучается использование контрольных выборок и перекрёстной проверки для более надёжной оценки моделей.

#### Кросс-валидация
----

Неверный способ проверки модели
---

Продемонстрируем наивный подход к проверке с помощью данных Iris. Начнём с загрузки данных: 

``` python
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data
y = iris.target
```

Далее выбираем модель и гиперпараметры. Будем использовать классификатор $k$-ближайших соседей с $n_{neighbors}=1$. Это очень простая и интуитивно понятная модель, которая гласит: «Метка неизвестной точки совпадает с меткой ближайшей к ней обучающей точки».

``` python
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=1)
```

Обучаем модель и используем ее для прогнозирования меток данных, которые мы уже знаем:

``` python
model.fit(X, y)
y_model = model.predict(X)
```

Вычисляем долю правильно помеченных точек:

``` python
from sklearn.metrics import accuracy_score
accuracy_score(y, y_model)
```

В результате мы получили показатель точности $1.0$, который указывает на то, что $100\%$ точек были правильно помечены нашей моделью! Но действительно ли это соответствует ожидаемой точности? Действительно ли мы получили модель, которая будет правильной в $100\%$ случаев?

Как вы, наверное, поняли, ответ отрицательный. На самом деле, этот подход содержит фундаментальный недостаток: он обучает и оценивает модель на основе одних и тех же данных. Кроме того, модель ближайшего соседа -- это инструмент оценки, который просто сохраняет обучающие данные и предсказывает метки, сравнивая новые данные с этими сохраненными примерами: за исключением неизвестных ему примеров, он каждый раз будет получать $100\%$ точность!

Holdout
----

Итак, что же можно сделать? Более полное представление о производительности модели можно получить, используя так называемый резервный набор (holdout set): мы сохраняем некоторое подмножество данных, полученных при обучении модели, а затем используем этот резервный набор для проверки производительности модели. Это разделение можно выполнить с помощью утилиты `train_test_split` в Scikit-Learn:

``` python
from sklearn.cross_validation import train_test_split
# split the data with 50% in each set
X1, X2, y1, y2 = train_test_split(X, y, random_state=0,
 train_size=0.5)

# fit the model on one set of data
model.fit(X1, y1)

# evaluate the model on the second set of data
y2_model = model.predict(X2)
accuracy_score(y2, y2_model)
```

Мы получаем более разумный результат: классификатор ближайших соседей примерно на $90\%$ точен на holdout set. Нoldout set похож на неизвестные данные, поскольку модель не "видела" его раньше

K-fold
----

Одним из недостатков использования резервного набора для проверки модели является то, что мы теряем часть наших данных при обучении модели. В приведенном выше случае половина набора данных не используется для обучения модели! Это неоптимально и может вызвать проблемы, особенно если исходный набор обучающих данных невелик.

Одним из способов решения этой проблемы является использование перекрестной проверки (cross-validation), то есть выполнение последовательности сопоставлений, при которой каждое подмножество данных используется как в качестве обучающего набора, так и в качестве набора для проверки. Визуально это может выглядеть примерно так:

@img02(holdout.png, Holdout set, 60)

В данном случае проводятся два проверочных испытания, попеременно используя каждую половину данных как holdout set. Используя разделённые ранее данные, мы могли бы реализовать это следующим образом:

``` python
y2_model = model.fit(X1, y1).predict(X2)
y1_model = model.fit(X2, y2).predict(X1)
accuracy_score(y1, y1_model), accuracy_score(y2, y2_model)
```

``` python
(0.95999999999999996, 0.90666666666666662)
```

В результате получаем два показателя точности, которые мы могли бы объединить (скажем, взяв среднее значение), чтобы получить более точную оценку производительности глобальной модели. Эта конкретная форма перекрестной проверки представляет собой двукратную перекрестную проверку (two-fold cross-validation), то есть ту, при которой мы разделили данные на два набора и использовали каждый из них по очереди в качестве набора для проверки.

Мы могли бы развить эту идею, чтобы использовать еще больше испытаний и увеличить объем данных — например, вот наглядное изображение пятикратной перекрестной проверки:

@img02(five-fold.png, Five-fold cv, 60)

В данном случае разделяем данные на пять групп и используем каждую из них по очереди для оценки соответствия модели остальным 4/5 данных. Это было бы довольно утомительно делать вручную, и поэтому мы используем процедуру `cross_val_score` в Scikit-Learn:

```python
from sklearn.cross_validation import cross_val_score
cross_val_score(model, X, y, cv=5)
```

``` python
array([ 0.96666667, 0.96666667, 0.93333333, 0.93333333, 1. ])
```

Повторная проверка на разных подмножествах данных позволяет получить ещё более полное представление об эффективности алгоритма. 

Leave-one-out
---

В Scikit-Learn реализован ряд полезных схем перекрёстной проверки, которые применимы в определённых ситуациях. Они реализованы с помощью итераторов в модуле `cross_validation`. Например, мы можем рассмотреть крайний случай, когда количество циклов кросс-валидации равно количеству точек данных: в каждом испытании модель обучается на всех точках, кроме одной. Этот тип кросс-валидации называется **leave-one-out** и может использоваться следующим образом:

``` python
from sklearn.cross_validation import LeaveOneOut
scores = cross_val_score(model, X, y, cv=LeaveOneOut(len(X)))
scores
```

Поскольку выборка содержит 150 примеров, метод leave-one-out даёт результаты для 150 испытаний: 1.0 -- правильное предсказание, 0.0 -- неправильное предсказание. Если взять среднее значение, можно получить оценку частоты ошибок:

``` python
scores.mean()
```

``` python
0.95999999999999996
```

Аналогичным образом можно использовать и другие схемы перекрестной проверки. Для получения описания того, что доступно в Scikit-Learn, используйте IPython для изучения подмодуля `sklearn.cross_validation` или ознакомьтесь с [онлайн-документацией Scikit-Learn по кросс-валидации](http://scikit-learn.org/stable/modules/cross_validation.html).

#### Выбор лучшей модели
---

Что делать если модель недостаточно эффективна? Существует несколько возможных ответов:

- Использовать более сложную/более гибкую модель
- Использовать менее сложную/менее гибкую модель
- Увеличить размер обучающей выборки
- Увеличить количество признаков

Ответ на этот вопрос часто противоречит здравому смыслу. В частности, иногда использование более сложной модели дает худшие результаты, а добавление большего количества обучающих выборок может не улучшить ваши результаты! Способность определить, какие шаги улучшат вашу модель, - это то, что отличает успешных практиков машинного обучения от неуспешных.

Bias-variance trade-off
----

По сути, вопрос о «лучшей модели» сводится к поиску оптимального компромисса между смещением и дисперсией (bias-variance trade-off). Рассмотрим следующую диаграмму, на которой показаны две регрессионные модели, построенные на одном и том же наборе данных:

@img02(bias-variance.png, Bias-variance trade-off, 80)

Ни одна из этих моделей не подходит к данным особенно хорошо, но они по-разному приводят к сбоям.

Модель слева пытается провести прямую линию между данными. Поскольку данные по своей сути более сложны, чем прямая линия, линейная модель никогда не сможет хорошо описать этот набор данных. Считается, что такая модель недообучена (**underfiting**): то есть она не обладает достаточной гибкостью, чтобы надлежащим образом учитывать все особенности данных; по-другому это можно объяснить тем, что модель имеет высокое смещение (high bias).

Модель справа пытается сопоставить данные с полиномом высокого порядка. Здесь модель обладает достаточной гибкостью, чтобы почти идеально учитывать мелкие особенности данных, но, несмотря на то, что она очень точно описывает обучающие данные, ее форма в большей степени отражает конкретные шумовые свойства данных, а не внутренние свойства процесса, генерирующего эти данные. Говорят, что такая модель переобучена (**overfiting**): то есть она обладает такой гибкостью, что в конечном итоге модель учитывает случайные ошибки, а также лежащее в ее основе распределение данных; по-другому можно сказать, что модель имеет высокую дисперсию (**high variance**).

Чтобы взглянуть на это с другой стороны, рассмотрим, что произойдет, если мы используем эти две модели для прогнозирования значения $y$ на новых данных. На следующих диаграммах красные точки указывают на данные, которые были исключены из обучающего набора:

@img02(underfit_overfit.png, Underfiting vs overfiting, 80)

В качестве оценки качества модели здесь выступает коэффициент детерминации $R^2$, который измеряет, насколько хорошо работает модель по сравнению с простым средним целевых значений. $R^2=1$ указывает на идеальное совпадение, $R^2=0$ означает, что модель работает не лучше, чем простое предсказание среднего значения данных, а отрицательные значения означают еще худшие модели. 

Из оценок, полученных для моделей выше, мы можем сделать вывод, который справедлив и в более общем плане:

- Для моделей с высоким смещением производительность модели на валидационной выборке аналогична производительности на обучающей выборке.
- Для моделей с высокой дисперсией производительность модели нна валидационной выборке намного хуже, чем на обучающей выборке.

Если мы представим, что у нас есть некоторая возможность настраивать сложность модели, мы ожидаем, что результаты обучения и проверки будут вести себя так, как показано на следующем рисунке:

@img02(valid_curve.png, Validation Curve Schematic, 60)

Представленную выше диаграмму часто называют **кривой валидации (validation curve)**.

- Оценка на обучающих данных везде выше, чем оценка на валидационных. Как правило, это так: модель будет лучше соответствовать данным, которые она видела, чем данным, которых она не видела.
- При очень низкой сложности модели (модель с высоким смещением) обучающие данные недостаточно соответствуют друг другу, что означает, что модель является плохим предиктором как для обучающих данных, так и для любых ранее невидимых данных.
- При очень высокой сложности модели (модели с высокой дисперсией) обучающие данные слишком хорошо соответствуют друг другу, что означает, что модель очень хорошо предсказывает обучающие данные, но не работает для любых ранее невидимых данных.
- Для некоторого промежуточного значения кривая проверки имеет максимум. Этот уровень сложности указывает на приемлемый компромисс между смещением и дисперсией.Способы настройки сложности модели варьируются от модели к модели.


Кривые валидации в Scikit-Learn
----

Рассмотрим пример использования кросс-валидации для вычисления кривой валидации для класса моделей. Будем использовать модель полиномиальной регрессии: это обобщенная линейная модель, в которой степень полинома является настраиваемым параметром. Например, полином $1$-й степени соответствует прямой линии, проходящей через данные; для параметров модели $a$ и $b$:

$$y=ax+b$$

Полином $3$-й степени соответствует кубической кривой, проходящей через данные; для параметров модели $a,b,c,d$:

$$y=ax^3+bx^2+cx+d$$

Это можно обобщить на любое количество полиномиальных признаков. В Scikit-Learn это реализуется с помощью простой линейной регрессии в сочетании с полиномиальной предобработкой. Для объединения этих операций используется `pipeline`.

``` python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline

def PolynomialRegression(degree=2, **kwargs):
    return make_pipeline(PolynomialFeatures(degree),
                         LinearRegression(**kwargs))
```

Теперь давайте создадим данные, на которых мы будем обучать нашу модель:

```python
import numpy as np

def make_data(N, err=1.0, rseed=1):
    # randomly sample the data
    rng = np.random.RandomState(rseed)
    X = rng.rand(N, 1) ** 2
    y = 10 - 1. / (X.ravel() + 0.1)
    if err > 0:
        y += err * rng.randn(N)
    return X, y

X, y = make_data(40)
```

Визуализируем наши данные и полиномиальные аппроксимации для нескольких степеней:

```python
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn; seaborn.set()  # plot formatting

X_test = np.linspace(-0.1, 1.1, 500)[:, None]

plt.scatter(X.ravel(), y, color='black')
axis = plt.axis()
for degree in [1, 3, 5]:
    y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)
    plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))
plt.xlim(-0.1, 1.0)
plt.ylim(-2, 12)
plt.legend(loc='best');
```

@img02(poly_regr.png, Polynomial Regression, 50)

Критерием, определяющим сложность модели, в данном случае является степень полинома, которая может быть любым неотрицательным целым числом. Полезно ответить на следующий вопрос: какая степень полинома обеспечивает приемлемый компромисс между смещением (underfitting) и дисперсией (overfitting)?

Для ответа на этот вопрос, визуализируем кривую валидации для этих конкретных данных и модели; это можно сделать, используя процедуру `validation_curve`, предоставляемую Scikit-Learn.

```python
from sklearn.learning_curve import validation_curve
degree = np.arange(0, 21)
train_score, val_score = validation_curve(PolynomialRegression(), X, y,
                                          'polynomialfeatures__degree', degree, cv=7)

plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')
plt.plot(degree, np.median(val_score, 1), color='red', label='validation score')
plt.legend(loc='best')
plt.ylim(0, 1)
plt.xlabel('degree')
plt.ylabel('score');
```

@img02(val_curve.png, Validation curve, 40)

Этот график показывает именно то качественное поведение, которого мы ожидали: оценка на обучающих данных везде выше, чем оценка на валидационных; оценка на обучающих данных монотонно улучшается с увеличением сложности модели; и оценка за валидацию достигает максимума, прежде чем упасть из-за переобучения модели.

Из данной кривой валидации можно сделать вывод, что для полинома третьего порядка найден оптимальный компромисс между смещением и дисперсией; мы можем вычислить и отобразить это соответствие исходным данным следующим образом:

```python
plt.scatter(X.ravel(), y)
lim = plt.axis()
y_test = PolynomialRegression(3).fit(X, y).predict(X_test)
plt.plot(X_test.ravel(), y_test);
plt.axis(lim);
```

@img02(poly_regr_final.png, Polynomial Regression, 40)

Обратите внимание, что поиск этой оптимальной модели на самом деле не требует вычисления оценки на обучающих данных, однако изучение взаимосвязи между оценкой на обучающих данных и оценкой на валидационных данных может дать полезную информацию о производительности модели.

#### Кривая обучения
---

Одним из важных аспектов сложности модели является то, что оптимальная модель, как правило, зависит от объема обучающих данных. 

Сгенерируем новый набор данных, в пять раз больше предыдущего:

```python
X2, y2 = make_data(200)
plt.scatter(X2.ravel(), y2);
```

@img02(data.png, Dataset, 50)

Продублируем предыдущий код, чтобы построить кривую проверки для этого большего набора данных; для справки давайте также перенесем предыдущие результаты:

```python
degree = np.arange(21)
train_score2, val_score2 = validation_curve(PolynomialRegression(), X2, y2,
                                            'polynomialfeatures__degree', degree, cv=7)

plt.plot(degree, np.median(train_score2, 1), color='blue', label='training score')
plt.plot(degree, np.median(val_score2, 1), color='red', label='validation score')
plt.plot(degree, np.median(train_score, 1), color='blue', alpha=0.3, linestyle='dashed')
plt.plot(degree, np.median(val_score, 1), color='red', alpha=0.3, linestyle='dashed')
plt.legend(loc='lower center')
plt.ylim(0, 1)
plt.xlabel('degree')
plt.ylabel('score');
```

@img02(val_curve1.png, Validation curve, 50)

На графике выше сплошные линии показывают новые результаты, пунктирные линии показывают результаты предыдущего меньшего набора данных. Из кривой валидации видно, что более крупный набор данных может поддерживать гораздо более сложную модель: пик здесь находится в районе $6$ степени полинома, но даже модель с $20$ степенью полинома не сильно искажает данные - результаты валидации и обучения остаются очень близкими.

Таким образом, мы видим, что поведение кривой валидации зависит не от одного, а от двух важных факторов: ~~сложности модели и количества точек обучения~~. Часто бывает полезно изучить поведение модели в зависимости от количества обучающих данных, что мы можем сделать, используя все более крупные подмножества данных для соответствия нашей модели. Зависимость обучающей/валидационной оценки от размера обучающей выборки называется **кривой обучения (learning curve)**.

Общее поведение кривой обучения:

- Модель заданной сложности будет переобучена на маленьком наборе данных: это означает, что оценка на обучающих данных будет относительно высокой, в то время как оценка на валидационных данных будет относительно низкой.
- Модель заданной сложности будет недообучена на большом наборе данных: это означает, что оценка на обучающих данных уменьшится, но оценка на валидационных данных увеличится.
- Модель практически никогда не даст более высокой оценки на валидационной выборке, чем на обучающей: это означает, что кривые должны постоянно сближаться, но никогда не пересекаться.

Учитывая эти особенности, мы ожидаем, что кривая обучения будет выглядеть качественно так, как показано на следующем рисунке:

@img02(learning_curve_schema.png, Learning curve schema, 50)

Характерной особенностью кривой обучения является приближение к определенному значению оценки по мере увеличения объема обучающей выборки. В частности, если вы собрали достаточное количество данных, чтобы модель сошлась (converged), добавление дополнительных обучающих данных вероятно никак не поможет! Единственный способ повысить производительность модели в этом случае - использовать другую (часто более сложную) модель.

Кривая обучения в Scikit-Learn
----

Scikit-Learn предлагает удобную утилиту для вычисления кривых обучения на основе ваших моделей.

Рассчитаем кривую обучения для нашего исходного набора данных с использованием полиномиальной модели второго порядка и полиномиальной модели девятого порядка:

``` python
from sklearn.learning_curve import learning_curve

fig, ax = plt.subplots(1, 2, figsize=(16, 6))
fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)

for i, degree in enumerate([2, 9]):
    N, train_lc, val_lc = learning_curve(PolynomialRegression(degree),
                                         X, y, cv=7,
                                         train_sizes=np.linspace(0.3, 1, 25))

    ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score')
    ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score')
    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1],
                 color='gray', linestyle='dashed')

    ax[i].set_ylim(0, 1)
    ax[i].set_xlim(N[0], N[-1])
    ax[i].set_xlabel('training size')
    ax[i].set_ylabel('score')
    ax[i].set_title('degree = {0}'.format(degree), size=14)
    ax[i].legend(loc='best')
```

@img02(learning_curves1.png, Learning curves, 90)

Это ценный диагностический инструмент, поскольку он дает наглядное представление о том, как модель реагирует на увеличение объема обучающих данных. В частности, когда ваша кривая обучения уже сходится (т.е. когда кривые обучения и валидации уже близки друг к другу), добавление дополнительных данных обучения не приведет к существенному улучшению! Эта ситуация показана на левом графике с кривой обучения для модели степени 2.

Единственный способ увеличить показатель сходимости - это использовать другую модель (обычно более сложную). Это видно на правом графике: переходя к гораздо более сложной модели, мы увеличиваем показатель сходимости (обозначенный пунктирной линией), но за счет более высокой дисперсии модели. Если бы мы добавили еще больше данных, кривая обучения для более сложной модели в конечном итоге сошлась бы.

Построение кривой обучения для выбранной вами конкретной модели и набора данных может помочь вам принять решение о том, как продвигаться дальше в улучшении вашего анализа.

Grid Search
---

На практике модели, как правило, имеют несколько "регуляторов", которые нужно поворачивать, от чего кривые валидации и кривые обучения переходят от прямых линий к многомерным поверхностям. В этом случае визуализация затруднена, и предпочтение отводится к простому поиску конкретной модели, максимизирующей валидационную оценку.

Scikit-Learn предоставляет автоматизированные инструменты для этого в модуле ~~grid search~~. Рассмотрим пример использования grid search для поиска оптимальной полиномиальной модели. Рассмотрим трехмерную сетку характеристик модели: 

- степень полинома;
- флаг, указывающий, следует ли обучать свободный член;
- флаг, указывающий, следует ли нормализовать данные.

Это можно настроить с помощью `GridSearchCV` в Scikit-Learn:

```python
from sklearn.grid_search import GridSearchCV

param_grid = {'polynomialfeatures__degree': np.arange(21),
              'linearregression__fit_intercept': [True, False],
              'linearregression__normalize': [True, False]}

grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)
```

Обратите внимание, что, как и при обычной оценке, это еще не применялось ни к каким данным. Вызов метода `fit()` приведет к обучению модели в каждой точке сетки, попутно отслеживая результаты:

```python
grid.fit(X, y);
```

Теперь мы можем запросить наилучшие параметры следующим образом:

```python
grid.best_params_
```

```python
{'linearregression__fit_intercept': False,
 'linearregression__normalize': True,
 'polynomialfeatures__degree': 4}
```

Можем использовать наилучшую модель и показать соответствие нашим данным, используя предыдущий код:

```python
model = grid.best_estimator_

plt.scatter(X.ravel(), y)
lim = plt.axis()
y_test = model.fit(X, y).predict(X_test)
plt.plot(X_test.ravel(), y_test, hold=True);
plt.axis(lim);
```

@img02(best_model.png, Best model, 50)


Grid search предоставляет гораздо больше возможностей, включая возможность задать пользовательскую функцию подсчета очков, распараллелить вычисления, выполнять рандомизированный поиск и многое другое. Для получения дополнительной информации обратитесь к документации [Scikit-Learn по grid search](http://scikit-learn.org/stable/modules/grid_search.html) или ознакомьтесь с подробными примерами:

- [In-Depth: Kernel Density Estimation](https://code.vt.edu/cci-ai-testbed/python-data-science-handbook/-/blob/cci/notebooks/05.13-Kernel-Density-Estimation.ipynb)
- [Feature Engineering: Working with Images](https://code.vt.edu/cci-ai-testbed/python-data-science-handbook/-/blob/cci/notebooks/05.14-Image-Features.ipynb)

