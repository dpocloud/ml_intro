<!--

author:     Alexander Trofimov
attribute:  **Editor:** Daria Gudkova
email:    
date:       21/02/2025
version:    0.1
language:   en
narrator:   English

icon:    ./icon.png
logo:    ./course_logo.png

comment:  Курс "Введение в машинное обучение"

import:   ./macro.md
          https://raw.githubusercontent.com/LiaScript/CodeRunner/master/README.md

link:     ./custom.css

-->

## Семинар. Основы предобработки данных.
-----

!?[Lecture03 Video](https://www.youtube.com/watch?v=8pTEmbeENF4)

В данном семинаре рассматриваются фундаментальные методы предобработки данных для машинного обучения: работа с пропущенными значениями, кодирование категориальных признаков, масштабирование и стандартизация данных.

Откройте ноутбук с семинаром для изучения на практике: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1uF7NTcqFQUmNaO4H2_IMm4DbxdqT8aeR?usp=sharing)

----
<h3 style="text-align: center;">Содержание</h3>

1. [Введение в предобработку данных](#введение)
2. [Работа с пропущенными значениями](#пропущенные-значения)
3. [Обработка категориальных данных](#категориальные-данные)
4. [Масштабирование и стандартизация](#масштабирование)
5. [Работа с текстовыми данными](#текстовые-данные)
6. [Проблема мультиколлинеарности](#мультиколлинеарность)
7. [Практические рекомендации](#практические-рекомендации)
8. [Итоговый тест](#итоговый-тест)
9. [Задания для самостоятельной работы](#задания)

----
### Зачем нужна предобработка данных
----


Предобработка данных является критически важным этапом в построении эффективных моделей машинного обучения. Реальные данные почти всегда содержат проблемы, которые необходимо решить перед обучением модели, в том числе:

- **Пропущенные значения** — отсутствующие данные в некоторых записях
- **Категориальные признаки** — нечисловые данные, требующие преобразования
- **Различные масштабы** — признаки с сильно отличающимися диапазонами значений
- **Избыточные признаки** — коррелирующие признаки, создающие мультиколлинеарность

Правильная предобработка может значительно улучшить производительность модели, в то время как неправильная — привести к плохим результатам даже при использовании сложных алгоритмов.

Основные этапы предобработки обычно содержат:

- Анализ качества данных
- Обработка пропусков
- Кодирование категорий
- Масштабирование
- Отбор признаков

@quizQuestion(`Что НЕ является типичной проблемой реальных данных?`)

<!-- data-randomize -->
[( )] Пропущенные значения
[( )] Категориальные признаки
[(X)] Слишком высокое качество данных
[( )] Различные масштабы признаков

----
### Работа с пропущенными значениями
----

Пропущенные данные — одна из самых распространённых проблем в реальных датасетах. Прежде чем выбрать метод обработки, важно понимать природу пропусков.

**Типы пропущенных данных**

| MCAR |MAR |MNAR |
|---|---|---|
|Missing Completely at Random|Missing at Random|Missing Not at Random|
|Пропуски случайны и не зависят от других данных | Пропуски зависят от наблюдаемых данных |Пропуски зависят от самих пропущенных значений |

Рассмотрим стратегии обработки пропущенных значений.

**Удаление записей**
---

Простейший подход — удалить все строки с пропущенными значениями.

```python
data_clean = data.dropna()
```

| ✅ Плюсы | ❌ Минусы |
|---------|----------|
| Простота реализации | Потеря информации |
| Сохранение структуры | Уменьшение выборки |
| Нет искажений | Возможное смещение |

**Заполнение фиксированной константой**
----

```python
# Заполнение нулями
data_zero = data.fillna(0)
```

| ✅ Плюсы | ❌ Минусы |
|---------|----------|
| Сохраняется размер выборки | Искажение распределения |
| Простота реализации | Может создать ложные закономерности |
| Может быть осмысленно для некоторых признаков | Не всегда логично (например, нулевой возраст) |

**Заполнение мерами центральной тенденции**
----

Стратегии заполнения:

- `'mean'` — среднее арифметическое
- `'median'` — медиана  
- `'most_frequent'` — мода

```python
from sklearn.impute import SimpleImputer

# Заполнение средним
imputer = SimpleImputer(strategy='mean')
data_filled = imputer.fit_transform(data)
```

| ✅ Плюсы | ❌ Минусы |
|---------|----------|
| Сохраняется размер выборки | Уменьшается дисперсия признака |
| Не создаёт выбросов| Может создать ложные закономерности |
| Логично для многих типов данных | Не учитывает связи между признаками |

**Интерполяция и более сложные методы**
---

Метод `interpolate()` заполняет пропуски, вычисляя промежуточные значения между известными точками по прямой линии.

Метод `fillna(method='ffill')` заполняет пропуски последним известным значением (распространяет данные "вперёд").

Метод `fillna(method='bfill')` заполняет пропуски следующим известным значением (распространяет данные "назад").

@quizQuestion(Сопоставьте когда какой метод использовать:)

<!-- data-randomize -->
- [(Интерполяция) (FFill) (BFill)]
- [ (x) ( ) ( )]  Значения меняются плавно и пропуски нужно заполнить "правдоподобными" промежуточными значениями
- [ ( ) (x) ( )]  Последнее известное значение остаётся актуальным (например, температура вчера ≈ температура сегодня утром)
- [ ( ) ( ) (x)]  Следующее значение уже известно и может быть применено ретроспективно

**Продвинутые методы**
----

**KNN-импутация**: заполнение на основе k ближайших соседей.

```python
from sklearn.impute import KNNImputer

knn_imputer = KNNImputer(n_neighbors=3)
data_knn = knn_imputer.fit_transform(data)
```

**Итеративная импутация**: множественная импутация с использованием итеративного алгоритма.

```python
from sklearn.impute import IterativeImputer
iterative_imputer = IterativeImputer(random_state=42)
data_iterative_filled = pd.DataFrame(
    iterative_imputer.fit_transform(advanced_data),
    columns=advanced_data.columns
)
```

| ✅ Плюсы | ❌ Минусы |
|---------|----------|
| Учитывают связи между признаками | Вычислительно более сложные |
| Более точные предсказания пропущенных значений| Могут переобучиться на малых выборках |
| Лучше сохраняют статистические свойства данных | Требуют настройки гиперпараметров|

@quizQuestion(`Какой метод заполнения пропусков лучше использовать для сильно скошенных данных?`)

<!-- data-randomize -->
[( )] Среднее арифметическое
[(X)] Медиана
[( )] Мода
[( )] Ноль

@quizQuestion(`В каком случае лучше удалить строки с пропусками?`)

<!-- data-randomize -->
[( )] Когда пропусков очень много (>80%)
[(X)] Когда пропусков мало (<5%) и выборка большая
[( )] Когда пропуски неслучайны
[( )] Всегда

---
### Обработка категориальных данных
---

Большинство алгоритмов машинного обучения работают только с числовыми данными, поэтому категориальные признаки необходимо преобразовать в числовой формат.

**Типы категориальных данных**:

- **Номинальные** — без естественного порядка. Примеры: цвет (красный, синий, зелёный), город, профессия.
- **Порядковые** — с естественным порядком. Примеры: размер (S, M, L, XL), образование (среднее, высшее).

Label Encoding
---

Присвоение каждой категории уникального числа.

```python
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
encoded = encoder.fit_transform(['Москва', 'СПБ', 'Казань'])
# Результат: [0, 1, 2]
```

| ✅ Плюсы | ❌ Минусы |
|---------|----------|
| Экономия памяти | Создаёт ложную упорядоченность для номинальных данных |
| Простота реализации | Может привести к неверным выводам модели |

**One-Hot Encoding**
---
Создание бинарных признаков для каждой категории.

```python
from sklearn.preprocessing import OneHotEncoder
import pandas as pd

# С pandas
encoded = pd.get_dummies(data, columns=['город'])

# С scikit-learn
encoder = OneHotEncoder(sparse_output=False)
encoded = encoder.fit_transform(data[['город']])
```

Пример преобразования:

```
Исходные данные:    
город
Москва
СПБ
Казань

После One-Hot:
город_Казань  город_Москва  город_СПБ
  0              1            0
  0              0            1
  1              0            0
```

| ✅ Плюсы | ❌ Минусы |
|---------|----------|
| Не создаёт ложной упорядоченности | Увеличивает размерность данных |
| Легко интерпретируется | Может создать разреженные матрицы |

**Target Encoding**
---

Замена категорий на среднее значение целевой переменной для этой категории.

```python
region_mean = data.groupby('регион')['продажи'].mean()
data['регион_encoded'] = data['регион'].map(region_mean)
```

| ✅ Плюсы | ❌ Минусы |
|---------|----------|
| Не увеличивает размерность, эффективно для категорий высокой кардинальности |Риск переобучения|
| Сохраняет информацию о связи с целевой переменной | Не работает без целевой переменной |

**Binary Encoding**
---

Комбинация Label Encoding и бинарного представления. Каждая категория кодируется в виде двоичного числа, а затем разбивается на отдельные биты (столбцы).

Преобразование категорий в числовые метки (Label Encoding):

- Каждой категории присваивается числовой код (начиная с $0$).  
- Например, для ['красный', 'синий', 'зелёный', ...] коды будут $[0, 1, 2, 3, 4, 5, 6, 7]$.  

```python
label_encoded = pd.Categorical(series).codes
```

Определение количества бит для кодирования: 

- Вычисляется минимальное число бит, достаточное для представления всех категорий.  
- Для 8 цветов: $log₂(8) = 3$ → нужно $3$ бита.  

```python
n_bits = int(np.ceil(np.log2(len(np.unique(label_encoded)))))
```

Разложение чисел в двоичный вид по битам

- Каждое число преобразуется в двоичное представление.  
- Например, число 5 ('оранжевый') в 3-битном виде: 101.  
- Каждый бит записывается в отдельный столбец:  
  - binary_0 (младший бит): 1 
  - binary_1: 0
  - binary_2 (старший бит): 1 

```python
for i in range(n_bits):
    binary_df[f'binary_{i}'] = (label_encoded >> i) & 1
```



Сравнение методов кодирования
----

| Метод | Номинальные | Порядковые | Высокая кардинальность |
|-------|-------------|------------|------------------------|
| Label Encoding | ❌ | ✅ | ✅ |
| One-Hot | ✅ | ✅ | ❌ |
| Target Encoding | ✅ | ✅ | ✅ |
| Binary Encoding| ✅ | ✅ | ✅ |

@quizQuestion(`Какой метод кодирования лучше использовать для признака "размер одежды" (XS, S, M, L, XL)?`)

<!-- data-randomize -->
[(X)] Label Encoding
[( )] One-Hot Encoding
[( )] Target Encoding
[( )] Binary Encoding

@quizQuestion(`В каком случае лучше использовать One-Hot Encoding?`)

<!-- data-randomize -->
[(X)] Когда категорий мало (<15) и данные номинальные
[( )] Когда категорий очень много (>50)
[( )] Когда важен естественный порядок категорий
[( )] Когда нужно сохранить размерность данных

@quizQuestion(`Когда стоит применять Binary Encoding?`)

<!-- data-randomize -->
[( )] Когда категорий меньше 5
[(X)] Когда категорий много (>15) и нужно уменьшить размерность
[( )] Когда важна интерпретируемость каждого бита
[( )] Когда данные порядковые

@quizQuestion(`В каком случае лучше выбрать Target Encoding?`)

<!-- data-randomize -->
[( )] Когда нет доступа к целевой переменной
[(X)] Когда категорий много и есть связь с целевой переменной
[( )] Когда нужно избежать любых рисков переобучения
[( )] Когда данные порядковые

@quizQuestion(`Когда можно использовать Label Encoding?`)

<!-- data-randomize -->
[( )] Для номинальных данных без естественного порядка
[(X)] Для порядковых данных с явной иерархией
[( )] Когда категорий больше 100
[( )] Когда нужно избежать ложной упорядоченности

@quizQuestion(`Какой метод не подходит для данных с высокой кардинальностью?`)

<!-- data-randomize -->
[( )] Binary Encoding
[( )] Target Encoding
[(X)] One-Hot Encoding
[( )] Hashing Encoding

---

### Масштабирование и стандартизация

В машинном обучении многие алгоритмы чувствительны к масштабу признаков. Если одни признаки измеряются в тысячах (например, зарплата), а другие — в единицах (например, возраст), это может исказить работу модели.

- Градиентный спуск сходится быстрее, когда признаки в одном масштабе.
- Метрики расстояния (KNN, кластеризация) могут искажаться из-за разных единиц измерения.
- Регуляризация в линейных моделях штрафует большие веса — без масштабирования некоторые признаки получат несправедливо большее влияние.

Масштабирование (Scaling) и стандартизация (Standardization) решают эту проблему, приводя данные к единому масштабу.

Стандартизация (Z-score)
---

Стандартизация приводит данные к распределению с $μ=0$ и $σ=1$.

$$X_{\text{standardized}}=\frac{X - μ}{σ}$$

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)
```

| Возраст | Зарплата (руб) | Standardization |
|---------|----------------|-----------------|
| 25      | 50 000         | [-1.22, -1.22]  |
| 30      | 100 000        | [0.0, 0.0]      |
| 35      | 150 000        | [1.22, 1.22]    |

| ✅ Плюсы | ❌ Минусы |
|---------|----------|
| Подходит для нормально распределённых данных | Не ограничивает диапазон значений|
| Устойчива к выбросам лучше, чем нормализация | Может быть неэффективна при сильно скошенных распределениях |

Min-Max нормализация
----

Приводит значения к диапазону $[0, 1]$.

$$X_{\text{scaled}} = \frac{X - X_{\min}}{X_{\max} - X_{\min}}$$

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
data_normalized = scaler.fit_transform(data)
```

| Возраст | Зарплата (руб) | Min-Max Scaling |
|---------|----------------|-----------------|
| 25      | 50 000         | [0.0, 0.0]      |
| 30      | 100 000        | [0.5, 0.5]      |
| 35      | 150 000        | [1.0, 1.0]      |

| ✅ Плюсы | ❌ Минусы |
|---------|----------|
| Ограниченный диапазон $[0, 1]$ |Чувствительна к выбросам|
| Сохраняет исходную форму распределения | Новые данные могут выйти за пределы [0, 1]|
|Подходит для данных с известными границами||

Робастное масштабирование
----

Робастное масштабирование — это метод стандартизации, устойчивый к выбросам. В отличие от обычной стандартизации (Z-score), которая использует среднее ($μ$) и стандартное отклонение ($σ$), RobustScaler опирается на медиану и межквартильный размах ($IQR$).

$$X_{\text{robust}} = \frac{X - \text{median}(X)}{\text{IQR}(X)}$$

где $IQR = Q3 - Q1$ (разница между 75-м и 25-м перцентилями).

```python
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
data_robust = scaler.fit_transform(data)
```

| Зарплата (руб) | Robust Scaling |
|----------------|----------------|
| 50 000         | -0.67          |
| 100 000        | 0.0            |
| 150 000        | 0.67           |
| **1 000 000**  | **5.33** (выброс не сломал масштаб) |

| ✅ Плюсы | ❌ Минусы |
|---------|----------|
| Устойчив к выбросам | Не гарантирует фиксированный диапазон |
| Подходит для любых распределений | Менее интерпретируем, чем Min-Max |

Когда использовать разные методы
---

| Метод | Когда использовать | Алгоритмы |
|-------|-------------------|-----------|
| **StandardScaler** | Нормально распределённые данные | Линейные модели, SVM, нейросети |
| **MinMaxScaler** | Данные с известными границами | KNN, нейросети |
| **RobustScaler** | Данные с выбросами | Любые алгоритмы |

@quizQuestion(`У вас есть признак "доход" с несколькими миллионерами в данных. Какой метод масштабирования выбрать?`)

<!-- data-randomize -->
[( )] StandardScaler
[( )] MinMaxScaler  
[(X)] RobustScaler
[( )] Никакой

@quizQuestion(`Для каких алгоритмов масштабирование критически важно?`)

<!-- data-randomize -->
[(X)] Метод k-ближайших соседей (KNN)  
[(X)] Метод опорных векторов (SVM)  
[( )] Дерево решений  
[(X)] Нейронные сети  

---
### Работа с изображениями
---

В компьютерном зрении изображения можно обрабатывать на разных уровнях абстракции, каждый из которых имеет свои преимущества и ограничения.

Использование значений пикселей
----

**Пример преобразования:**
Исходное изображение $8×8$ пикселей → Вектор из $64$ значений интенсивности

| ✅ Преимущества | ❌ Ограничения |
|----------------|----------------|
| Прямой доступ к исходным данным | Высокая размерность (например, $262144$ признаков для изображений $512×512$) |
| Простота реализации | Чувствительность к геометрическим преобразованиям (сдвигам, поворотам) |
| Не требует предварительной обработки | Не учитывает семантические особенности |


Извлечение высокоуровневых признаков
----

Для извлечения высокоуровневых признаков для изображений часто используются сверточные нейросети (CNN).

```python
from tensorflow.keras.applications import VGG16
model = VGG16(weights='imagenet', include_top=False)
features = model.predict(image)
```

| ✅ Преимущества | ❌ Ограничения |
|----------------|----------------|
| Автоматическое обучение признаков | Требует больших вычислительных ресурсов |
| Максимальное сохранение информации | Сложность интерпретации |
| Лучшее качество для сложных задач | Необходимость большого объема данных |

Сравнение методов
---

| Метод | Когда применять | Примеры использования |
|-------|----------------|-----------------------|
| **Пиксели** | Простые изображения, ограниченные ресурсы | MNIST, простые классификаторы |
| **Ручные признаки** | Специфичные задачи с известными паттернами | Обнаружение лиц, текстуры |
| **CNN-признаки** | Сложные задачи компьютерного зрения | Классификация ImageNet, медицинские изображения |

@quizQuestion(`Для задачи классификации рукописных цифр (MNIST) размером 28×28 какой метод предпочтителен?`)

<!-- data-randomize -->
[(X)] Прямое использование пикселей  
[( )] Извлечение HOG-признаков  
[( )] Использование предобученной CNN  
[( )] Все методы одинаково хороши  

@quizQuestion(`Какой главный недостаток использования сырых пикселей для изображений высокого разрешения?`)

<!-- data-randomize -->
[( )] Сложность реализации  
[(X)] Чрезмерно высокая размерность данных  
[( )] Невозможность обработки цветных изображений  
[( )] Требовательность к эксперным знаниям  

@quizQuestion(`Какое преимущество CNN перед ручным извлечением признаков?`)

<!-- data-randomize -->
[( )] Более простая реализация  
[(X)] Автоматическое обучение релевантных признаков  
[( )] Меньшие требования к вычислительным ресурсам  
[( )] Лучшая интерпретируемость результатов  

---
### Работа с текстовыми данными
---

Текстовые данные требуют специальной обработки перед использованием в алгоритмах машинного обучения. Сырой текст содержит много "шума": знаки препинания, различные формы слов, стоп-слова и другие элементы, которые могут мешать анализу.

**Основные этапы предобработки:**

- **Токенизация** — разбиение текста на отдельные слова
- **Очистка** — удаление ненужных символов и форматирования
- **Нормализация** — приведение к единому формату
- **Фильтрация** — удаление стоп-слов и редких слов
- **Морфологический анализ** — стемминг или лемматизация
- **Векторизация** — преобразование в числовой формат


Токенизация
---

**Токенизация** — это процесс разбиения текста на отдельные единицы (токены), обычно слова или подслова.

```python
def simple_tokenize(text):
    """Простая токенизация по пробелам"""
    return text.split()

def advanced_tokenize(text):
    """Продвинутая токенизация с обработкой знаков препинания"""
    # Удаляем знаки препинания (кроме дефисов внутри слов)
    text = re.sub(r'[^\w\s-]', ' ', text)
    # Разбиваем на токены и убираем пустые
    tokens = [token.strip() for token in text.split() if token.strip()]
    return tokens
```

Исходный текст: "Машинное обучение — это область искусственного интеллекта!"

Простая токенизация: "['Машинное', 'обучение', '—', 'это', 'область', 'искусственного', 'интеллекта!']"

Продвинутая токенизация: "['Машинное', 'обучение', 'это', 'область', 'искусственного', 'интеллекта']"

Очистка и нормализация текста
---

**Очистка** включает удаление ненужных символов, а **нормализация** — приведение к единому формату.

```python
def clean_and_normalize(text):
    """Очистка и нормализация текста"""
    # Приведение к нижнему регистру
    text = text.lower()

    # Удаление лишних пробелов
    text = re.sub(r'\s+', ' ', text)

    # Удаление специальных символов (оставляем только буквы, цифры, пробелы и дефисы)
    text = re.sub(r'[^а-яё\w\s-]', ' ', text)

    # Убираем лишние пробелы после очистки
    text = text.strip()

    return text
```

До:    "Машинное обучение — это область искусственного интеллекта!"

После: "машинное обучение   это область искусственного интеллекта"

Удаление стоп-слов
---

**Стоп-слова** — это часто встречающиеся слова, которые обычно не несут смысловой нагрузки (предлоги, союзы, местоимения).

```python
stopwords = {'и', 'в', 'не', 'что', 'на', 'с', 'как', 'а', 'но'}

def remove_stopwords(tokens):
    return [word for word in tokens if word not in stopwords]
```

Стемминг и лемматизация
---

**Стемминг** — это процесс отсечения окончаний для приведения слов к корневой основе (стему).

```python
def russian_stemmer(word):
    # Пример типичных русских окончаний
    suffixes = [
        'ому', 'ему', 'ой', 'ей', 'ая', 'яя', 'ое', 'ее', 'ые', 'ие', 'ый', 'ий', 'ая', 'ов', 'ам', 'ого',
        'ах', 'ми', 'ем', 'ом', 'ет', 'ит', 'ал', 'ел', 'ил', 'ол', 'ул', 'ы', 'и', 'а', 'о', 'е', 'у'
    ]

    word = word.lower()

    # Минимальная длина стема
    min_stem_length = 3

    for suffix in suffixes:
        if word.endswith(suffix) and len(word) - len(suffix) >= min_stem_length:
            return word[:-len(suffix)]

    return word
```

До:    ['машинное', 'обучение', 'область', 'искусственного', 'интеллекта']

После: ['машин', 'обуч', 'область', 'искусствен', 'интеллект']

**Лемматизация** — процесс приведения слова к его словарной форме (лемме) с учетом морфологии языка.

```python
def simple_lemmatize(word):
    """Лемматизация на основе словаря"""
    lemma_dict = {
        'данные': 'данное', 'данных': 'данное',
        'обучение': 'обучение', 'обучения': 'обучение', 'обучать': 'обучать',
        'машинный': 'машинный', 'машинного': 'машинный', 'машинное': 'машинный',
        'алгоритмы': 'алгоритм', 'алгоритм': 'алгоритм',
        'модели': 'модель', 'моделей': 'модель', 'модель': 'модель',
        'анализируют': 'анализировать', 'находят': 'находить',
        'являются': 'являться', 'улучшает': 'улучшать',
        'критически': 'критический', 'важный': 'важный'
    }
    return lemma_dict.get(word.lower(), word.lower())
```

Векторизация
---

**Векторизация** — преобразование текста в числовые векторы для использования в алгоритмах машинного обучения.

1. **Bag of Words**

Текст представляется в виде «мешка» из разных слов, при этом игнорируется порядок слов, а важна только частота, с которой они встречаются. 

Для каждого текста создаётся вектор, где каждый элемент описывает количество вхождений определённого слова из словаря. 

```python
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
bow_matrix = vectorizer.fit_transform(texts)
```

Некоторые области применения:

- Классификация документов. Частотность вхождения слова используется как признак для обучения классификатора.
- Фильтрация спама. Сообщения электронной почты моделируются неупорядоченным набором слов, выбранных из одного из двух распределений вероятностей: спама и обычных электронных писем. 
- Анализ настроений. Модель широко применяется в анализе отзывов клиентов: из отзывов создаётся «мешок» слов, и бизнес анализирует настроения, связанные с определёнными продуктами или услугами. 

У метода есть ограничения:

- не учитывает порядок слов, что может привести к потере информации
- не берёт в расчёт семантическую близость слов
- редкие, но важные для понимания смысла слова могут быть «задавлены» часто встречающимися словами. 

2. **TF-IDF (Term Frequency-Inverse Document Frequency)**

TF-IDF вычисляет важность каждого слова в документе относительно количества его употреблений в данном документе и во всей коллекции текстов. Этот метод позволяет выделить ключевые слова и понять, какие слова имеют больший вес для определенного документа в контексте всей коллекции.

TF (Частота термина) обозначает, насколько часто определенное слово появляется в данном документе. Таким образом, TF измеряет важность слова в контексте отдельного документа.

IDF (Обратная частота документа) измеряет, насколько уникально слово является по всей коллекции документов. Слова, которые появляются в большинстве документов, имеют низкое IDF, так как они не вносят большой информационной ценности.

$$TFIDF(t, d) = TF(t, d) * IDF(t)$$

где:

$TF(t, d)$ - Частота слова $t$ в документе $d$.

$IDF(t)$ - Обратная частота документа для слова $t$.

```python
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(texts)
```

| ✅ Преимущества | ❌ Ограничения |
|----------------|----------------|
| Учет значимости слов | Вычислительно сложнее BoW |
| Эффективен для поиска | Все ещё игнорирует контекст |
| Хорош для задач ранжирования | |


3. **Word Embeddings**

Наиболее продвинутый способ векторизации текстов: получение эмбеддингов. Слова представляются в виде векторов низкой размерности (эмбеддингов). Если слова близки по смыслу, их векторы будут похожи, т.е. в многомерном пространстве близкие по семантике слова расположены ближе друг к другу. Для вычисления близости слов используется косинусное расстояние между их векторами.

Чтобы добиться такого результата, эмбеддинги обучают на больших массивах текстов с использованием нейронных сетей. 

@img02(king_queen.png, Word2vec, 50)

| ✅ Преимущества | ❌ Ограничения |
|----------------|----------------|
| Эмбеддинги способны улавливать семантические отношения между словами — векторные представления слов «кот» и «кошка» будут похожими | Сложен в реализации |
| Вектора получаются относительно короткими, особенно по сравнению с прямым кодированием | Требует больших вычислительных ресурсов, чтобы обучить нейросеть |
| Слова в эмбеддингах можно преобразовывать с помощью векторных операций, например «король» – «мужчина» + «женщина» ≈ «королева» | Точность эмбеддингов сильно зависит от качества и размера обучающего корпуса текстов |

**Word2Vec** -- одна из популярных технологий получения эмбеддингов.

Технология Word2Vec сочетает две архитектуры:

- Continuous Bag-of-Words (CBOW) предсказывает нужное слово на основе его контекста, то есть окружающих слов;

- Skip-gram предсказывает контекст на основе целевого слова.

Обе архитектуры обучаются на большом текстовом корпусе с использованием нейронной сети. В ходе обучения веса нейронной сети изменяются таким образом, чтобы слова, которые часто встречаются в одном контексте, имели близкие векторы.

Скажем, если слова «кошка» и «собака» часто встречаются в похожих контекстах вроде «домашнее животное», «лает/мяукает», то векторы этих слов будут близки друг к другу.

Сравнение методов векторизации текста
---

| Метод | Плюсы | Минусы | Применение |
|-------|-------|--------|------------|
| **Bag of Words** | Простота, скорость | Не учитывает порядок | Классификация документов |
| **TF-IDF** | Учитывает важность слов | Больше вычислений | Поиск, ранжирование |
| **Word Embeddings** | Семантическая близость | Сложность обучения | NLP задачи |

@quizQuestion(`Что показывает высокое значение IDF для слова?`)

<!-- data-randomize -->
[(X)] Слово редко встречается в коллекции документов
[( )] Слово часто встречается в документе
[( )] Слово не важно для анализа
[( )] Слово является стоп-словом

@quizQuestion(`Когда лучше использовать Bag of Words?`)

<!-- data-randomize -->
[(X)] Для простой классификации текстов  
[( )] Для задач машинного перевода  
[( )] Когда важно учитывать семантические связи  
[( )] Для работы с короткими сообщениями в соцсетях  

---

### Проблема мультиколлинеарности

Мультиколлинеарность возникает, когда признаки сильно коррелируют друг с другом.

**Проблемы:**

- Неустойчивость коэффициентов модели
- Сложность интерпретации
- Переобучение

Обнаружение мультиколлинеарности
---

1. **Корреляционная матрица**

```python
correlation_matrix = data.corr()
# Высокие корреляции (>0.8) - признак проблемы
```

2. **VIF (Variance Inflation Factor)**


```python
from statsmodels.stats.outliers_influence import variance_inflation_factor

def calculate_vif(df):
    vif = pd.DataFrame()
    vif["признак"] = df.columns
    vif["VIF"] = [variance_inflation_factor(df.values, i) 
                  for i in range(df.shape[1])]
    return vif
```

Интерпретация VIF:

- VIF < 5: Низкая мультиколлинеарность 
- 5 ≤ VIF < 10: Умеренная
- VIF ≥ 10: Высокая

**Удаление коррелирующих признаков**
---

```python
def remove_correlated_features(df, threshold=0.8):
    corr_matrix = df.corr().abs()
    upper_triangle = corr_matrix.where(
        np.triu(np.ones_like(corr_matrix, dtype=bool), k=1))
    
    to_drop = [column for column in upper_triangle.columns 
               if any(upper_triangle[column] > threshold)]
    
    return df.drop(columns=to_drop)
```

**PCA (Principal Component Analysis)**
---

В основе PCA лежит идея нахождения новых признаков, называемых главными компонентами, которые максимально коррелируют с исходными данными и при этом ортогональны друг другу. Эти главные компоненты формируют новый базис в пространстве признаков, исключая лишнюю информацию и снижая размерность.

Прежде чем приступить к вычислению главных компонент, важно стандартизировать данные, приводя их к нулевому среднему и единичной дисперсии. Это важно, потому что признаки с разными масштабами могут исказить результаты PCA.

Существует несколько методов для оценки оптимального числа компонент, включая **метод объясненной дисперсии**: 

Этот метод заключается в выборе числа компонент так, чтобы доля объясненной дисперсии достигла заданного порога (например, 95% или 99%). Это позволяет сохранить большую часть информации при снижении размерности.

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=0.95)  # Сохранить 95% дисперсии
data_pca = pca.fit_transform(data_scaled)
```

После снижения размерности и выбора оптимального числа компонент, становится важным понять, что представляют собой эти компоненты. Интерпретация главных компонент может помочь в понимании того, какие признаки они кодируют и какие зависимости между признаками они выделяют.

Для интерпретации главных компонент можно анализировать их веса (собственные векторы) и связанные с ними признаки. Например, в случае анализа изображений, можно выяснить, что первая главная компонента может быть связана с освещенностью изображений, а вторая - с ориентацией объектов.

Интерпретация главных компонент зависит от конкретной задачи и данных, над которыми вы работаете. Это может потребовать дополнительного анализа и доменных знаний для полного понимания значения главных компонент

**Регуляризация**
----

```python
from sklearn.linear_model import Ridge, Lasso

# Ridge (L2) - уменьшает коэффициенты
ridge = Ridge(alpha=1.0)

# Lasso (L1) - обнуляет коэффициенты
lasso = Lasso(alpha=0.1)
```

Сравнение методов устранения мультиколлиниарности
----

| Метод | Когда использовать | Плюсы | Минусы |
|-------|-------------------|-------|--------|
| **Удаление признаков** | Явно избыточные признаки | Простота | Потеря информации |
| **PCA** | Нужно снизить размерность | Сохраняет дисперсию | Потеря интерпретируемости |
| **Ridge** | Все признаки важны | Сохраняет все признаки | Не убирает полностью |
| **Lasso** | Автоматический отбор | Отбор признаков | Может быть неустойчивым |

@quizQuestion(`Что НЕ является типичной проблемой реальных данных?`)

<!-- data-randomize -->

    [[?]] Какой метод лучше использовать, если нужно сохранить интерпретируемость модели?
    [(X)] Удаление коррелирующих признаков
    [( )] PCA
    [( )] Регуляризация
    [( )] Все методы одинаково хороши

---

### Частые ошибки и как их избежать

Утечка данных при масштабировании
---

```python
# НЕПРАВИЛЬНО:
scaler.fit(X_all)  # Используем все данные!
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ПРАВИЛЬНО:
scaler.fit(X_train)  # Только обучающие данные!
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

Новые категории в тестовых данных
---

```python
# ПРАВИЛЬНО:
encoder = OneHotEncoder(handle_unknown='ignore')
# Новые категории будут закодированы как вектор нулей
```

Несохранение параметров предобработки
---

```python
import joblib

# Сохраняем обученный предобработчик
joblib.dump(preprocessor, 'preprocessor.pkl')

# Загружаем для новых данных
preprocessor = joblib.load('preprocessor.pkl')
new_data_processed = preprocessor.transform(new_data)
```

### Чек-лист предобработки

- **Изучить данные**

  - Анализ структуры и статистик
  - Проверка распределений (гистограммы, boxplot)
  - Поиск аномалий и выбросов

- **Разделить на train/test/val**

  - Стратифицированное разбиение для несбалансированных данных
  - Фиксация random_state для воспроизводимости

- **Обработать пропуски**

  - Анализ природы пропусков (MCAR, MAR, MNAR)
  - Выбор метода: удаление, среднее/медиана, предсказание, интерполяция (для временных рядов)

- **Обработать категориальные признаки**

  - Разделение на номинальные/порядковые
  - Выбор кодирования: One-Hot, Target, Binary для номинальных; Label Encoding для порядковых

- **Масштабировать числовые признаки**

  - StandardScaler для нормальных распределений
  - RobustScaler при наличии выбросов
  - MinMaxScaler для нейросетей и ограниченных диапазонов

- **Проверить мультиколлинеарность**

  - Матрица корреляций
  - Расчет VIF (Variance Inflation Factor)
  - Удаление/объединение коррелирующих признаков

- **Обработать выбросы**

  - IQR-метод
  - Z-score (для нормальных распределений)
  - Трансформации (логарифмирование)

- **Feature Engineering**

  - Создание новых признаков

- **Валидация**

  - Контроль утечек данных
  - Кросс-валидация
  - Проверка распределений в train/test

- **Сохранение параметров**

  - Масштабировщики
  - Кодировщики
  - Импутеры
  - Для применения в продакшене

@quizQuestion(Какой принцип является САМЫМ важным при предобработке данных?)

<!-- data-randomize -->
[( )] Использовать самые сложные методы
[(X)] Избегать утечки данных из тестовой выборки
[( )] Всегда применять все методы предобработки
[( )] Удалять как можно больше признаков

---

### ✅ Тест по разделу

@quizQuestion(`У вас есть признак "возраст" с пропусками. Данные сильно скошены вправо (много молодых людей). Какой метод заполнения выбрать?`)

<!-- data-randomize -->
[( )] Среднее арифметическое
[(X)] Медиана
[( )] Мода
[( )] Константа (например, 0)

@quizQuestion(`Для какого типа категориальных данных подходит Label Encoding?`)

<!-- data-randomize -->
[( )] Номинальные данные (цвет, город)
[(X)] Порядковые данные (размер, рейтинг)
[( )] Любые категориальные данные
[( )] Только текстовые данные

@quizQuestion(`Вы обучаете модель KNN на данных с признаками: "возраст" (20-80 лет) и "доход" (20,000-200,000 руб). Что произойдет без масштабирования?`)

<!-- data-randomize -->
[( )] Модель будет работать корректно
[(X)] Признак "доход" будет доминировать из-за большего масштаба
[( )] Признак "возраст" будет доминировать
[( )] Никаких проблем не возникнет

@quizQuestion(`Какое значение VIF указывает на серьёзную проблему мультиколлинеарности?`)

<!-- data-randomize -->
[( )] VIF = 2
[( )] VIF = 5
[(X)] VIF = 15
[( )] VIF = 0.5

@quizQuestion(`Что из перечисленного является признаком утечки данных?`)

<!-- data-randomize -->
[( )] Использование кросс-валидации
[(X)] Масштабирование всех данных перед разделением на train/test
[( )] Применение One-Hot Encoding
[( )] Удаление выбросов

@quizQuestion(`В методе TF-IDF высокое значение IDF означает, что слово:`)

<!-- data-randomize -->
[(X)] Редко встречается в коллекции документов
[( )] Часто встречается в текущем документе
[( )] Является стоп-словом
[( )] Содержит много символов

@quizQuestion(`Какой метод лучше использовать для работы с выбросами при масштабировании?`)

<!-- data-randomize -->
[( )] StandardScaler
[( )] MinMaxScaler
[(X)] RobustScaler
[( )] Никакой

---

### Задания для самостоятельной работы

#### Задание 1: Анализ пропущенных значений ⭐

**Условие:** У вас есть датасет с информацией о клиентах банка:

- `возраст`: 15% пропусков
- `доход`: 8% пропусков  
- `семейное_положение`: 5% пропусков
- `кредитная_история`: 3% пропусков

**Задачи:**

1. Проанализируйте природу пропусков (случайные или нет)
2. Выберите подходящую стратегию для каждого признака
3. Обоснуйте свой выбор
4. Реализуйте обработку пропусков

**Подсказка:** Рассмотрите корреляции между пропусками в разных признаках.

#### Задание 2: Кодирование категориальных данных ⭐⭐

**Условие:** Датасет интернет-магазина содержит:

- `категория_товара`: 50 уникальных значений
- `бренд`: 200 уникальных значений
- `размер`: XS, S, M, L, XL, XXL
- `цвет`: 15 уникальных значений
- `рейтинг_качества`: плохой, средний, хороший, отличный

**Задачи:**

1. Определите тип каждой категориальной переменной
2. Выберите подходящий метод кодирования для каждой
3. Реализуйте кодирование
4. Сравните размерность данных до и после обработки

**Бонус:** Попробуйте Target Encoding для признаков с высокой кардинальностью.

#### Задание 3: Комплексная предобработка ⭐⭐⭐

**Условие:** Датасет для предсказания цен на недвижимость:

- Числовые: площадь, количество комнат, этаж, год постройки
- Категориальные: район, тип здания, состояние
- Целевая переменная: цена

**Задачи:**

1. Создайте полный пайплайн предобработки
2. Обработайте пропуски (добавьте их искусственно)
3. Закодируйте категориальные признаки
4. Примените масштабирование
5. Проверьте на мультиколлинеарность
6. Создайте несколько версий признаков (например, цена за кв.м.)

**Критерии оценки:**

- Корректность выбора методов
- Избежание утечки данных
- Качество документирования кода
- Валидация результатов

#### Задание 4: Анализ текстовых данных ⭐⭐

**Условие:** Коллекция отзывов о фильмах на русском языке.

**Задачи:**

1. Реализуйте полную предобработку текстов:

   - Очистку и нормализацию
   - Токенизацию
   - Удаление стоп-слов
   - Стемминг

2. Создайте векторизацию тремя методами:

   - Bag of Words
   - TF-IDF
   - Word2Vec (если есть возможность)

3. Сравните результаты векторизации
4. Проанализируйте наиболее важные слова для каждого класса отзывов

**Бонус:** Добавьте анализ n-грамм (биграммы, триграммы).

#### Задание 5: Исследовательский проект ⭐⭐⭐⭐

**Тема:** "Влияние методов предобработки на качество модели"

**Задачи:**

1. Выберите публичный датасет (Kaggle, UCI ML Repository)
2. Создайте несколько вариантов предобработки:

   - Базовый (минимальная обработка)
   - Стандартный (по рекомендациям из семинара)
   - Продвинутый (с использованием дополнительных техник)

3. Обучите простую модель (например, логистическую регрессию) на каждом варианте
4. Сравните результаты и сделайте выводы
5. Подготовьте отчёт с рекомендациями

**Структура отчёта:**

- Описание данных и задачи
- Обоснование выбора методов предобработки
- Сравнительный анализ результатов
- Выводы и рекомендации

---

### Полезные ресурсы

- [Scikit-learn User Guide: Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)
- [Feature Engineering for Machine Learning](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/)
- [Hands-On Machine Learning, Chapter 2](https://github.com/ageron/handson-ml2)