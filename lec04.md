<!--

author:     Alexander Trofimov
attribute:  **Editor:** Daria Gudkova
email:    
date:       21/02/2025
version:    0.1
language:   en
narrator:   English

icon:    ./icon.png
logo:    ./course_logo.png

comment:  Курс "Введение в машинное обучение"

link:     ./custom.css

import:   ./macro.md

-->


## Диагностика регрессионных моделей
-----

!?[Lecture04 Video](https://www.youtube.com/watch?v=8pTEmbeENF4)

---
<h3 style="text-align: center;">Содержание раздела</h3>

- **Проверка предположений регрессии**

  - Проверка линейности
  - Проверка гомоскедастичности и независимости
  - Проверка нормальности

- **Обнаружение выбросов и влиятельных наблюдений**

  - Регрессионные выбросы
  - Мера влияния и расстояние Кука

- **Обнаружение мультиколлинеарности**

  - Проблема мультиколлинеарности
  - Регуляризованная регрессия
  - Отбор признаков

----
### Основы регрессионной диагностики
----

Понятие регрессионной диагностики
----

Вспомним этапы машинного обучения:

1. **Предобработка данных** — очистка, нормализация, кодирование
2. **Отбор признаков** — выбор наиболее информативных переменных
3. **Обучение модели** — подбор параметров модели
4. **Диагностика модели** — проверка качества и адекватности построенной модели

**Регрессионная диагностика** — это часть регрессионного анализа, цель которой состоит в том, чтобы исследовать, являются ли обученная модель и предположения, которые мы сделали о данных и модели, ~~согласованными с наблюдаемыми данными~~.

**Задачи регрессионной диагностики:**

- Проверка адекватности ~~предположений регрессионного анализа~~
- Обнаружение экстремальных точек (~~выбросов~~), которые могут доминировать в регрессии и искажать результаты
- Обнаружение сильных взаимосвязей между независимыми переменными (~~коллинеарности~~), влияющих на результаты
- Оценка структуры модели — правильно ли выбран класс модели (например, для нелинейной зависимости выбрали линейную модель и получили высокое смещение)

**Подходы к регрессионной диагностике:**

- **Графический анализ** — визуальная оценка с помощью различных графиков
- **Количественный анализ** — статистические тесты и численные меры

Основные предположения линейной регрессии
----

**Модель линейной регрессии:**
$$Y|x = x\beta + \varepsilon(x)$$
где $x = (1, x_1, ..., x_k)$, $\beta = (\beta_0, ..., \beta_k)^T$

**Предположения в линейном регрессионном анализе:**

1. **Линейность** — регрессионная функция линейна
2. **Экзогенность регрессоров** — объясняющие переменные независимы от ошибок
3. **Гомоскедастичность** — постоянная дисперсия ошибок
4. **Независимость ошибок** — ошибки не коррелированы между собой
5. **Нормальность** — ошибки имеют нормальное распределение
6. **Вариабельность регрессоров** — независимые переменные не являются константами

Только при выполнении этих предположений метод наименьших квадратов дает оптимальные оценки. Если предположения нарушены, то статистические выводы могут быть невалидными.

@NB(`Применение МНК для линейных моделей оптимально только когда выполнены все эти предположения.`)

----
### Проверка предположений регрессии
----

----
#### Проверка линейности
----

**Предположение:** регрессионная функция является линейной $\varphi(x) = x\beta$

**Графические методы диагностики:**

- Графики остатков — анализ остатков относительно предсказанных значений или переменных. Если на графиках остатков мы видим неслучайные паттерны или закономерности, это говорит о нарушении предположения о линейности. Например, если истинная зависимость квадратичная (парабола), а мы аппроксимируем её линейной функцией, то в остатках мы получим параболу — ту часть, которую линейная модель не смогла объяснить.
- Частичные регрессионные графики — анализ связи между переменными при исключении влияния других переменных.

**Количественные методы диагностики:**

- Тесты функциональной формы модели (тест Рамсея на ошибку спецификации регрессии и др.)

**Решения при нарушении линейности:**

- Преобразование переменных (добавление квадратичных членов, логарифмирование)
- Добавление новых информативных признаков
- Переход к нелинейным моделям регрессии

В простой регрессии нелинейность очевидна, и графиков остатков обычно достаточно для выявления нелинейности. В множественной регрессии нелинейность может быть ~~замаскирована~~. Индивидуальные двумерные графики не учитывают эффект других объясняющих переменных в модели. 

Например, нелинейность может быть скрыта в нескольких переменных: в проекциях на отдельные переменные зависимость линейна, а зависимость от нескольких переменных нелинейна. Или наоборот. Таким образом, проекции в множественной регрессии могут быть неинформативны.

@img04(resid4.png, Residual Plot, 50)

@img04(resid5.png, Residual Plot, 50)

Смещение пропущенной переменной (OVB)
----

Влияния на зависимую переменную $Y$, которые не улавливаются моделью, ~~собираются в ошибке~~, которая, как мы предполагали, не коррелирует с регрессорами. Если в модели пропущена переменная, влияющая на результат $Y$ и связанная с существующими регрессорами, то ~~предположение об экзогенности нарушается~~. Смещение МНК-оценок из-за этой ~~неправильной спецификации модели~~ называется **смещением пропущенной переменной (omitted variable bias, OVB)**.

**Негативные эффекты OVB:**

- МНК-оценки становятся ~~смещенными~~ → неверная интерпретация регрессионных коэффициентов
- МНК-оценки становятся ~~несостоятельными~~ → OVB нельзя решить увеличением числа наблюдений

Истинная модель:
$$Y|x=\beta_0+\beta_1x_1+\beta_2x_2+\varepsilon(x)$$
где $\varepsilon(x)\sim N(0,\sigma^2)$

Хорошая модель:
$$Y|x=\beta_0+\beta_1x_1+\beta_2x_2+\varepsilon_1(x)$$
МНК-оценки $\hat\beta_0,\hat\beta_1,\hat\beta_2$ будут несмещенными и состоятельными

Недоспецифицированная модель:
$$Y|x=\beta_0+\beta_1x_1+\varepsilon_2(x)$$
Значение ошибки $\varepsilon_2(x)$ будет зависеть от $x_2$.

- $x_1$ и $x_2$ независимы $\Rightarrow\varepsilon_2(x)$ и $x_1$ независимы → ~~нет смещения~~
- $x_1$ и $x_2$ коррелированы $\Rightarrow\varepsilon_2(x)$ и $x_1$ коррелированы → экзогенность нарушена $\Rightarrow\hat\beta_0,\hat\beta_1$ ~~смещены~~ и несостоятельны

📋 **Пример. Положительное смещение.** 

Пусть зарплата $зависит$ от образования $education$ и навыков $skills$.

Истинная модель: $$salary = \beta_0 + \beta_1 education + \beta_2 skills + \varepsilon(x)$$

Допустим, мы не учли регрессор $skills$.

Недоспецифицированная модель (Under-specified model): $$salary = \beta_0 + \beta_1 education + \varepsilon_1(x)$$

Корреляции: 
$$\rho(education, skills) > 0$$ 
$$\rho(skills, salary) > 0$$ 
$$\rho(education, salary) > 0$$

МНК-оценка $\hat\beta_1$ будет **переоценена** (overestimated) (положительное смещение), поскольку:

- Часть влияния $skills$ на $salary$ перейдёт к коэффициенту при $education$ 
- Метод наименьших квадратов завысит коэффициент $\beta_1$
- Мы можем сделать неправильные выводы о силе влияния образования на зарплату

📋 **Пример. Отрицательное смещение.** 

Пусть оценка $score$ по математике зависит от времени $gametime$, потраченного школьником на игры, и времени $studytime$, потраченного на обучение.

Истинная модель: $$score=\beta_0+\beta_1gametime+\beta_2studytime+\varepsilon(x)$$

Допустим, мы не учли регрессор $studytime$.

Недоспецифицированная модель (Under-specified model): $$score=\beta_0+\beta_1gametime+\varepsilon_1(x)$$

Корреляции: 
$$\rho(gametime, studytime) < 0$$
$$\rho(studytime, score) > 0$$

МНК-оценка $\hat\beta_1$ будет **недооценена** (underestimated) (отрицательное смещение), поскольку:

- $gametime$ влияет на $studytime$ отрицательно (чем больше играют, тем меньше учатся)
- $studytime$ влияет на $score$ положительно
- Получается опосредованное влияние: $gametime \rightarrow studytime \rightarrow score$. Из истинной модели мы могли бы получить положительные коэффициенты $\beta_1, \beta_2$, а в недоспецифицированная модели получим отрицательный $\beta_1$.

Мы можем сделать неправильный вывод: чем больше времени школьник играет, тем хуже его оценки. Однако, при фиксированном времени учёбы, время игр может даже положительно влиять на оценки (развивающие игры, тренировка реакции и т.д.).

📋 **Пример. Проблема корреляции признаков.** 
Пусть зарплата $зависит$ от образования $education$ и навыков $skills$. Коэффициент корреляции между $salary$ и $skills$ равен 0.9, между $salary$ и $education$ тоже 0.9. Но если $education$ уже учтён, то какую дополнительную информацию несёт $skills$? Если $education = skills$ (полностью коррелированы), то $skills$ не несёт новой информации.

Для того, чтобы вычислить чистое влияние одной переменной на другую (без опосредованных влияний) и выбрать признаки используется частичный регрессионный график (partial regression plot).

Частичный регрессионный график
----

Исследуем чистое влияние переменной $x_j$ на $Y$, исключив влияние всех остальных переменных.

Регрессия $Y$ на объясняющие переменные без $x_j$:
$$\hat{y} = X_{\sim j}\hat{\beta} + \varepsilon_Y$$

Регрессия $X_j$ на другие объясняющие переменные:
$$\hat{x}_j = X_{\sim j}\tilde{\beta} + \varepsilon_X$$
где $X_{\sim j}$ -- матрица дизайна с исключенным регрессором $x_j, j=\overline{0,k}$

Диаграмма рассеяния $\varepsilon_Y$ vs $\varepsilon_X$ называется **частичным регрессионным графиком (partial regression plot)** или **графиком добавленной переменной (added variable plot)**.

- $\varepsilon_Y$ представляет часть значений отклика, не объясненную предикторами (кроме $x_j$)
- $\varepsilon_X$ представляет часть значений $x_j$, не объясненную другими предикторами
- Подогнанная линия в плоскости $\varepsilon_Y$ vs $\varepsilon_X$ показывает, как новая информация от добавления $x_j$ может объяснить необъясненную часть значений отклика

**Применение частичных регрессионных графиков:**

- Выявление гетероскедастичности
- Обнаружение влиятельных точек данных
- Определение необходимости включения регрессора в существующую модель
- Определение потребности в нелинейных преобразованиях данных

**Возможные результаты анализа:**

Предположим, что у нас уже есть регрессионная модель $Y$ на $x_2$ и рассмотрим, следует ли нам добавить $x_1$ в модель.

@img04(add1.png, Partial Regression Plot, 70)

**a) Горизонтальная линия:** $x_1$ не содержит никакой дополнительной информации, полезной для предсказания $Y$, кроме той, которая уже содержится в $x_2$. Включение $x_1$ неоправданно.

**b) Линейная зависимость:** $x_1$ содержит полезную дополнительную информацию для предсказания $Y$. Включение в модель оправдано.

**c) Нелинейная зависимость:** Включение $x_1$ оправдано, но требуется нелинейное преобразование (квадратичный член, логарифм и т.д.).

Графики частичной регрессии полезны для выявления гетероскедастичности, важных точек данных, необходимости включения регрессора в существующую модель и необходимости нелинейного преобразования данных.

Если наклон подогнанной линии на графике частичной регрессии близок к нулю, а доверительные границы включают горизонтальную линию, то новая информация из $x_j$ не объясняет необъяснимую часть значения ответа. То есть, $x_j$ не имеет существенного значения в соответствии с моделью

Графики частичной регрессии при множественной линейной регрессии играют ту же роль, что и точечные диаграммы при простой линейной регрессии. Это единственный инструмент для многомерного случая, позволяющий увидеть чистое влияние каждого регрессора.

Некоторые другие графики (график частичных остатков, график CCPR) связаны с графиком частичной регрессии.

📋 **Пример.** 

Пусть престиж (prestige) зависит от дохода (income) и образования (education).

Регрессионная модель $prestige \sim 1 + income + education$

@img04(add2.png, Partial Regression Plot, 70)

Мы видим, что prestige линейно зависит от income (при фиксированном education) и от education (при фиксированном income). Влияние каждой переменной чистое, несмотря на то, что income и education сами между собой коррелированы.

----
#### Проверка гетероскедастичности и независимости
----

Проверка гетероскедастичности
----

**Предположение:** дисперсия ошибок $\mathrm{D}[\varepsilon(x)] = \sigma^2, \forall x \in 𝒳$

**Графические методы диагностики:**

- Графики остатков -- анализ остатков относительно предсказанных значений или переменных.

Визуальные признаки гетероскедастичности:

- Воронкообразная форма — дисперсия увеличивается с ростом предсказанных значений
- Обратная воронка — дисперсия уменьшается с ростом предсказанных значений
- Другие систематические паттерны в изменении разброса остатков

@img04(heter2.png, Heteroscedasticity of Residuals, 50)

**Статистические тесты гетероскедастичности:**

- Тест Уайта
- Тест Бройша-Пагана
- Тест Парка
- Тест Глейзера
- Тест Голдфельда-Квандта

Идея статистических тестов:

1. Строим основную регрессионную модель и получаем остатки $\varepsilon_i$
2. Строим вспомогательную модель: зависимость модуля остатков или квадрата остатков от регрессоров:
   $$|\varepsilon_i| = \gamma_0 + \gamma_1 x_{i1} + ... + \gamma_k x_{ik} + u_i$$
3. Проверяем значимость этой вспомогательной модели:

   - Если модель значима → есть гетероскедастичность (дисперсия меняется)
   - Если модель незначима → гомоскедастичность (дисперсия постоянна)

@img04(heter1.png, Heteroscedasticity of Residuals, 50)

**Решения при гетероскедастичности:**

- Преобразование переменных (логарифмирование, корень)
- Взвешенный метод наименьших квадратов
- Использование робастных стандартных ошибок

Экзогенность регрессоров
----

**Линейная модель:**
$$Y|x=x\beta+\varepsilon(x)$$

**Предположение:** независимая переменная $X$ и ошибка $\varepsilon(x)$ являются независимыми $\forall x \in 𝒳$. Это означает, что выборочные значения объясняющей переменной не зависят от ошибки модели. Это ключевое условие для получения несмещённых МНК-оценок.

Предположение об экзогенности не может быть проверено без дополнительной информации о наблюдаемых переменных (о природе данных и способе их сбора).

Для проверки экзогенности существуют специальные техники:

- Метод инструментальных переменных
- Тесты на эндогенность

📋 **Пример. Нарушение экзогенности.** 

Временной ряд, где текущий регрессор зависит от предыдущих значений:
$$Y|x_i=x_i\beta+\varepsilon(x_i)$$
где $x_i=y_{i-1}$, $\varepsilon(x_i)$ -- случайная ошибка, не являющаяся экзогенной, поскольку $x_i$ зависят от ошибки $\varepsilon(x_{i-1})$.

Независимость ошибок
----

**Предположение:** любая пара ошибок $\varepsilon(x_i)$ и $\varepsilon(x_j)$ (или $Y|x_i$ и $Y|x_j$) независимы.

Более слабое предположение -- некоррелированность:
$$\mathrm{cov}[\varepsilon(x_i), \varepsilon(x_j)] = \mathrm{cov}[Y|x_i, Y|x_j] = 0 \quad \forall x_i, x_j \in 𝒳, i \neq j$$

На практике проверяют более слабое условие -- **отсутствие автокорреляции**, то есть корреляции между соседними остатками. Если автокорреляция отсутствует, это ещё не гарантирует независимость, но является необходимым условием.

**Графические методы диагностики:**

- Residual Lag Plot — анализ корреляции между остатками с задержкой

Построение Lag Plot: 

- По оси $X$: остаток для наблюдения $i-1$ (предыдущий)
- По оси $Y$: остаток для наблюдения $i$ (текущий)
- Строим диаграмму рассеяния $\varepsilon_i$ vs $\varepsilon_{i-1}$

Интерпретация Lag Plot:

- Нет зависимости (точки разбросаны хаотично) → автокорреляция текущего остатка с предыдущим отсутствует
- Линейная зависимость → сильная автокорреляция текущего остатка с предыдущим

На практике ограничиваются проверкой только первого лага. Однако, для полной проверки независимости нужно построить lag plots для всех лагов:

- $\varepsilon_i$ vs $\varepsilon_{i-1}$ (лаг 1)
- $\varepsilon_i$ vs $\varepsilon_{i-2}$ (лаг 2)
- $\varepsilon_i$ vs $\varepsilon_{i-3}$ (лаг 3)
- ...

**Статистические тесты автокорреляции** cтроят вспомогательную регрессию остатка на предыдущие остатки и проверяют её значимость.

- Durbin-Watson test -- проверяет автокорреляцию первого порядка
- Ljung-Box test -- проверяет автокорреляцию нескольких порядков
- Breusch-Godfrey test -- более общий тест

📋 **Пример независимых остатков.**

@img04(lag3.jpg, Residual Lag Plot, 50)

График показывает хаотичный разброс точек без видимой закономерности -- автокорреляция отсутствует.

📋 **Пример сильно коррелированных остатков.**

@img04(lag1.png, Residual Lag Plot, 50)

График показывает почти линейную зависимость между текущим и предыдущим остатками. Это говорит об очень сильной автокорреляции.
Причина автокорреляции: если остатки образуют гладкую функцию, то соседние остатки не могут быть произвольными -- они обязаны быть близкими к предыдущим значениям.

**Решения при автокорреляции:**

- Преобразование переменных (отбеливающее преобразование)
- Обобщенный метод наименьших квадратов
- Переход к главным компонентам

----
#### Проверка нормальности
----

**Предположение:** ошибки $\varepsilon(x) \sim N(0, \sigma^2), \forall x \in 𝒳$

Нормальность остатков необходима для выполнения условий **теоремы Гаусса-Маркова**, которая утверждает, что МНК-оценки являются оптимальными среди всех возможных способов нахождения коэффициентов $\beta_0, \beta_1, ..., \beta_k$.

**Графические методы диагностики:**

- Гистограмма остатков -- визуальная оценка формы распределения
- Q-Q график (квантильная диаграмма) -- сравнение квантилей выборки с квантилями нормального распределения

**Статистические тесты нормальности:**

- Тесты согласия (тест хи-квадрат, тест Колмогорова-Смирнова)
- Тест Харки-Бера
- Тест Шапиро-Уилка

**Решения при нарушении нормальности:**

- Преобразование переменных (логарифмирование, степенные преобразования)
- Обобщенные линейные модели
- Робастные методы регрессии

Q-Q график 
----

Для графического сравнения двух распределений вероятностей (например, эмпирического и теоретического) используется Q-Q график.

Пусть $X \sim F(x)$ и $Y \sim G(y)$. $q$-е квантили:
$$x_q = F^{-1}(q), \quad y_q = G^{-1}(q), \quad q \in (0,1)$$

**Q-Q (квантиль-квантиль) кривая** -- это параметрическая кривая $\{(x_q, y_q)\}$.

**Алгоритм построения:**

1. Выбираем уровни квантилей, например: $q = 0.01, 0.02, ..., 0.99$ (с шагом $0.01$)
2. Для каждого уровня $q$:

   - Находим теоретический квантиль стандартного нормального распределения: $x_q = \Phi^{-1}(q)$
   - Находим выборочный квантиль: точку $y_q$, левее которой находится доля $q$ всех наблюдений

3. Наносим точки $(x_q, y_q)$ на график
4. Анализируем расположение точек относительно референсной линии

Если два распределения одинаковы, то их квантили нарастают одинаково → точки лежат на прямой линии под углом $45°$.

@img04(qq3.png, Q-Q Plot, 35)

Если $F(x)$ — нормальное распределение, а $G(y)$ — выборочное распределение, то Q-Q график называется **normal probability plot**. Математическое ожидание и дисперсия нормального распределения $F(x)$ обычно неизвестны, тогда используются выборочное среднее и выборочная дисперсия, $X\sim N(\bar x, S^2)$.

@img04(qq1.png, Normal Probability Plot, 50)

График Q-Q обычно строится с помощью **референсной линии (reference line)**. Библиотечные функции позволяют нанести референсную линию несколькими способами:

1. Соединение квартилей: Линия через точки, соответствующие квантилям $0.25$ и $0.75$
2. Линия регрессии: Регрессия выборочных квантилей на теоретические
3. Линия $y = x$: Для сравнения с идеальным случаем

@img04(qq4.png, Reference Line on Q-Q Plot, 50)

**Интерпретация Q-Q графика:**

- Каждая точка на графике Q-Q соответствует определенному квантилю, полученному из обоих распределений.
- Если сравниваемые распределения похожи, точки на графике Q-Q будут приблизительно лежать на прямой $y=x$.
- Если распределения линейно связаны, точки на графике Q-Q будут приблизительно лежать на одной прямой, но не обязательно на линии $y=x$.
- Точки, расположенные ниже референсной линии слева (или выше референсной линии справа), указывают на более тяжелый хвост (больше выбросов), чем при нормальном распределении.
- Более пологая референсная линия указывает на то, что распределение выборки имеет широкие хвосты и положительный эксцесс. Более крутая референсная линия указывает на то, что распределение выборки имеет узкие хвосты и отрицательный эксцесс.
- Выбросы на графике Q-Q соответствуют выбросам в выборке.


График Q-Q, как правило, является более эффективным методом сравнения распределений, чем сравнение гистограмм.

1. ~~Нет гиперпараметров~~. Для гистограммы нужно выбирать число столбцов

   - Слишком много столбцов → изрезанная картина, трудно интерпретировать
   - Слишком мало столбцов → потеря информации о форме распределения

2. ~~Работает с малыми выборками~~. Q-Q график информативен даже при $20$ точках, а для гистограммы нужно много данных для накопления статистики в каждой группе

@img04(qq11.png, Normal Probability Plot, 50)

**Типичные паттерны нарушения нормальности:**

|Правосторонняя асимметрия (right skewed)|Левосторонняя асимметрия (left skewed)| Under-dispersed data (thin tails)|Over-dispersed data (fat tails)|
|---|---|---|---|
|Длинный правый хвост, коэффициент асимметрии $> 0$|Длинный левый хвост, коэффициент асимметрии $< 0$|Отрицательный эксцесс, данные сконцентрированы вокруг среднего|Положительный эксцесс, много выбросов|
|Кривая выпукла вниз|Кривая выпукла вверх|S-образная кривая|Обратная S-образная кривая|
|При малых квантилях данные плотные (медленный рост по $Y$), при больших квантилях данные разрежены (быстрый рост по $Y$)|Сначала быстрый рост (разреженные данные в левом хвосте), затем медленный (плотные данные справа)|В хвостах плотность больше, чем у нормального распределения|В хвостах плотность меньше, чем у нормального распределения|
|@img04(qq12.png, Normal Probability Plot, 100)|@img04(qq13.png, Normal Probability Plot, 100)|@img04(qq14.png, Normal Probability Plot, 100)|@img04(qq15.png, Normal Probability Plot, 100)|

----
### Обнаружение выбросов и влиятельных наблюдений
----

----
#### Регрессионные выбросы
----

**Регрессионный выброс** — это наблюдение с большим остатком, то есть оно имеет необычное значение зависимой переменной $Y$ при данных значениях объясняющих переменных $x_1, ..., x_k$.

**Проблемы, вызываемые выбросами:**

1. Искажение коэффициентов — линия регрессии может "притягиваться" к выбросу
2. Нарушение предположения о нормальности — если выбросов много при достаточном объёме данных, это может указывать на распределение с тяжёлыми хвостами
3. Ухудшение качества предсказаний — модель подстраивается под аномальные наблюдения
4. Неслучайные паттерны в остатках — появление зависимостей, которых не должно быть

**Причины появления выбросов:**

- Ошибки при вводе данных (например, при регистрации)
- Особые условия наблюдения, не типичные для генеральной совокупности
- Распределение с тяжёлыми хвостами (не нормальное)
- Наличие неучтённых факторов для конкретных наблюдений

@NB(`Выбросы могут иметь большое значение остатка, но не обязательно влияют на оцененный наклон или свободный коэффициент. Влияние выброса зависит от его положения в пространстве регрессоров.`)

**Графические методы диагностики:**

- Графики стандартизированных остатков
- Ящичные диаграммы (box plots)

Стандартизированные остатки имеют распределение Стьюдента $T(n-k-1)$.
Стандартизированные остатки получаются путём деления обычных остатков на их стандартное отклонение. Если предположения регрессии выполнены, то стандартизированные остатки должны находиться в интервале примерно от $-3 \sigma$ до $+3 \sigma$. Значения вне этого диапазона могут указывать на выбросы.

@img04(outlier2.png, Standartized Residual Plot, 40)

**Меры выбросности:**

- Расстояние Кука — измеряет влияние наблюдения на всю регрессию
- DFBETAS — изменение коэффициентов при удалении наблюдения
- DFFITS — изменение предсказанного значения при удалении наблюдения

**Решения при обнаружении выбросов:**

1. Убедиться, что выброс не является ошибкой ввода
2. Проанализировать причины, почему наблюдение аномально
3. Исключить выбросы, если есть веские основания
4. Использовать робастные методы регрессии, -- методы, устойчивые к выбросам
5. Преобразовать данные (логарифмирование может сгладить выбросы)

Box Plot
----

**Box Plot** или **box-and-whisker diagram (ящичная диаграмма)** — это стандартизированный способ отображения распределения данных на основе пяти статистик:

- "Минимум" ($Q1 - 1.5×IQR$)
- Первый квартиль ($Q1$) — $25$-й процентиль
- Медиана — $50$-й процентиль
- Третий квартиль ($Q3$) — $75$-й процентиль
- "Максимум" ($Q3 + 1.5×IQR$)

где $IQR = Q3 - Q1$ (интерквартильный размах)

@img04(box0.png, Box plot, 60)

**Элементы ящичной диаграммы:**

- Ящик — от $Q1$ до $Q3$, содержит $50\%$ данных
- Линия в ящике — медиана
- Усы — протягиваются до самых удаленных точек в пределах $1.5×IQR$
- Выбросы (Outliers) — точки за пределами усов

Нужно быть осторожным при интерпретации box plot, особенно если данные имеют распределение с тяжёлыми хвостами. В таких случаях много точек может оказаться за границами усов, но это не означает, что все они являются выбросами — это может быть особенностью распределения. Поэтому не стоит автоматически исключать все точки, выходящие за границы усов.

Leverage и влияние
----

Наблюдения с экстремальными значениями предикторов имеют **высокий leverage (рычаг)**. Точки с высоким leverage ~~не обязательно влияют~~ на оцененный коэффициенты модели регрессии.
Рассмотрим несколько ситуаций:

1. **Выброс "в стороне"** — точка имеет аномальное значение $Y$, но находится в центральной области значений $X$. Такой выброс может не сильно повлиять на наклон регрессионной прямой.

2. **Выброс "на краю"** — точка находится на краю диапазона значений $X$. Даже небольшое отклонение в $Y$ может существенно изменить наклон линии регрессии.

3. **"Безобидный" выброс** — точка с аномальным $Y$, но которая лежит примерно на той же линии, что и остальные данные. Такой выброс практически не влияет на коэффициенты модели.

Поэтому при анализе выбросов нас интересуют в первую очередь **влиятельные наблюдения** — те, которые существенно изменяют оценки коэффициентов при их исключении.

@img04(lever1.png, Leverage, 40)

Представьте, что у вас есть рычаг для подъёма тяжести. Чем дальше от точки опоры вы прикладываете силу, тем больше рычаг. Аналогично в регрессии: чем дальше точка находится от "центра" распределения регрессоров, тем больше её потенциальное влияние на модель.

Расстояние Кука
----

**Расстояние Кука** — мера влияния $i$-го наблюдения на регрессию, показывающая, насколько изменятся предсказанные значения при удалении этого наблюдения.

**Формула:**
$$D_i = \frac{\sum_{l=1}^n(\hat{y}_l - \hat{y}_{l,\sim i})^2}{(k+1)S_e^2}, \quad i = \overline{1,n}$$
где 
$\hat{y}_l$ — предсказанное значение для $l$-го наблюдения в полной модели, 
$\hat{y}_{l,\sim i}$ — предсказанное значение для $l$-го наблюдения в модели без $i$-го наблюдения, 
$k+1$ — число параметров модели (включая свободный член)

Расстояние Кука $D_i$ показывает влияние $i$-го наблюдения на предсказанные значения отклика. Чем больше $D_i$, тем сильнее влияние наблюдения:

- $D_i > 1$ — наблюдение сильно влияет на регрессию, требует внимания
- $D_i > 4/n$ — альтернативный критерий для больших выборок

Казалось бы, для расчёта расстояния Кука нужно $n$ раз пересчитывать модель (по одному разу для каждого исключённого наблюдения). Это вычислительно затратно, особенно при больших $n$.

**Альтернативная формула:**
$$D_i = \frac{\hat{\varepsilon}_i^2}{(k+1)S_e^2} \frac{h_i}{(1-h_i)^2}, \quad i = \overline{1,n}$$
где $\hat{\varepsilon}_i$ — остаток, $h_i$ — leverage $i$-го наблюдения.

Эта формула позволяет вычислить все расстояния Кука за один проход, используя остатки и leverage от исходной модели. Не нужно многократно переобучать модель! Можно заметить, что расстояние Кука максимально при :

- большом остатке $\hat{\varepsilon}_i^2$ (точка плохо описывается моделью)
- высоком leverage $h_i$ (точка находится на краю распределения регрессоров)

Максимальное влияние имеют точки, которые одновременно являются выбросами и имеют экстремальные значения регрессоров.

За выбросы принимают наблюдения с расстоянием Кука больше некоторого порога $D_0$. Рекомендуемый порог: $$D_0 = 3\bar{D}$$где $\bar{D} = \frac{1}{n}\sum_{i=1}^n D_i$

Таким образом, расстояние Кука измеряет, насколько далеко "сместится" вся поверхность регрессии при удалении одного наблюдения. Если наблюдение находится в области с малой плотностью других точек и имеет большой остаток, его влияние будет максимальным.

Построим график расстояний Кука для всех наблюдений:

- По оси X — номер наблюдения
- По оси Y — расстояние Кука

@img04(cook1.png, Cook's distance, 40)

Большинство точек должно быть близко к нулю. Точки, которые "выстреливают" вверх, требуют внимания — это потенциально влиятельные наблюдения.

----
### Обнаружение мультиколлинеарности
----

Проблема мультиколлинеарности
----

Объясняющие переменные $x_1, ..., x_k$ называются **мультиколлинеарными**, если они линейно связаны:
$$c_0 + c_1x_{1i} + ... + c_kx_{ki} = 0 \quad \forall i = \overline{1,n}$$
где $c_0, c_1, ..., c_k$ — константы и $c_1^2 + ... + c_k^2 > 0$

Другими словами, мультиколлинеарность означает, что одну из переменных можно выразить через остальные. Например, если $x_3 = 2x_1 + 3x_2$, то переменные мультиколлинеарны.

Совершенная мультиколлинеарность редка на практике, но высокая мультиколлинеарность приводит к **потере статистического решения**:

- Большие стандартизированные остатки
- Широкие доверительные интервалы
- Низкие значения $t$-статистик, высокие $p$-значения в тестах гипотез
- Огромная чувствительность к небольшим изменениям в данных и спецификации модели

Модель множественной линейной регрессии: $$Y|x=x\beta+\varepsilon(x)$$

МНК-оценки: $$\hat\beta=(X^TX)^{-1}X^Ty$$
где $X$ -- design matrix:
$$X=\left(\begin{matrix}
1   & x_{11} & ... & x_{k1} \\
... & ... & ... & ...  \\
1   & x_{1n} & ... & x_{kn} \\
\end{matrix}\right)$$

Если $x_1, ..., x_k$ ~~совершенно мультиколлинеарны (perfectly multicollinear)~~, то по крайней мере один из столбцов $X$ является ~~линейной комбинацией~~ других:
$$\text{rank}(X^TX) = \text{rank}(X) < k+1$$
и матрица $X^TX$ ~~не обратима~~.

В этом случае система нормальных уравнений $(X^TX)\beta = X^Ty$ не имеет единственного решения.

📋 **Пример. Парадоксальный эффект мультиколлинеарности.**

Пусть оценка коэффициента $\hat{\beta}_1 = 100$ (большое значение). Коэффициент детерминации $R^2 = 0.9$ (модель хорошо объясняет данные). А при проверке гипотезы $H_0: \beta_1 = 0$ мы получаем $p$-value близкое к $1$, то есть гипотеза принимается. Это кажется противоречивым, но типично для мультиколлинеарности: большие оценки коэффициентов при плохих статистических свойствах.

📋 **Пример. Нарушение вариабельности регрессоров.**

Пусть в выборке значение регрессора константны x_i=x_0. 

1) Двумерный случай. Все точки данных лежат на вертикальной линии в пространстве регрессоров. В таком случае регрессионная модель может быть построена бесконечно многими вариантами: любая прямая, проходящая через центр, дает минимальное значение СКО по МНК. Добавление одной точки в стороне кардинально изменит наклон плоскости.

@img04(mult1.png, Variability of Regressor, 40)

2) Многомерный случай. Регрессионная модель может быть построена бесконечно многими вариантами: любая плоскость, проходящая через центр, дает минимальное значение СКО по МНК. Добавление одной выбросной точки кардинально изменит наклон плоскости. Данные не заполняют всё пространство регрессоров, а лежат в подпространстве меньшей размерности.

@img04(mult3.png, Multicollinearity, 40)

Nearly Multicollinearity
----

Переменные $x_1, ..., x_k$ ~~почти мультиколлинеарны(nearly multicollinear)~~, если:
$$c_0 + c_1x_{1i} + ... + c_kx_{ki} + \nu_i = 0 \quad \forall i = \overline{1,n}$$
где $\nu_i$ — случайный шум.

Для почти мультиколлинеарных регрессоров матрица $X^TX$ имеет обратную, но она ~~плохо обусловлена~~, так что алгоритмы численной инверсии могут быть нестабильными, т.е. небольшое изменение элементов матрицы приводит к большому изменению аппроксимированной обратной матрицы.

Чтобы оценить степень линейной зависимости столбцов design матрицы X используется **число обусловленности матрицы $A$:**
$$\kappa(A) = \frac{|\lambda_{\max}|}{|\lambda_{\min}|}$$
где $\lambda_{\min}, \lambda_{\max}$ — минимальное и максимальное собственные значения $A$.

$\kappa(A) \gg 1 \Leftrightarrow A$ плохо обусловлена.

Диагностика мультиколлинеарности
----

**Графические методы диагностики:**

- Диаграммы рассеяния между объясняющими переменными (Scatter plot)

Scatter plot могут помочь обнаружить только парную корреляцию, но мультиколлинеарность — более широкое понятие. Возможна ситуация, когда все парные коэффициенты корреляции близки к нулю, корреляционная матрица выглядит "идеально", но при этом матрица $X^TX$ плохо обусловлена или вообще не обращается. Это происходит потому что парные корреляции показывают только проекции на двумерные подпространства, а мультиколлинеарность может проявляться только в полном многомерном пространстве.

**Количественные методы диагностики:**

- Парные корреляции между объясняющими переменными, $R^2$
- Число обусловленности $X^TX$
- Меры мультиколлинеарности (VIF, толерантность)
- Тест Фаррара-Глаубера (F-G тест)

**Решения:**

- Преобразование объясняющих переменных (PCA, ICA)
- Регуляризованные методы регрессии
- Регрессия частичных наименьших квадратов (PLS)

Variance Inflation Factor (VIF)
----

**Идея:** если множественная линейная регрессия $X_j$, $j=\overline{1,k}$, на все другие объясняющие переменные имеет высокий коэффициент детерминации $R_j^2$, то между $x_1, ..., x_k$ существует мультиколлинеарность.

**Variance Inflation Factor (VIF) для $X_j$:**
$$VIF_j = \frac{1}{1-R_j^2}, \quad j = \overline{1,k}$$
где $R_j^2$ — коэффициент детерминации модели множественной регрессии:
$$X_j|x_{\sim j} = x_{\sim j}\beta_{\sim j} + \varepsilon(x_{\sim j})$$
где $x_{\sim j}$ — вектор регрессоров с исключенным $x_j$.

Высокие значения любого $VIF_j$ указывают на мультиколлинеарность. Чем выше $VIF_j$, тем сильнее мультиколлинеарность.

- $VIF_j = 1$ — переменная $X_j$ ортогональна всем остальным (нет мультиколлинеарности)
- $VIF_j > 1$ — переменная $X_j$ коррелирована с другими переменными
- $VIF_j > 5$ — умеренная мультиколлинеарность
- $VIF_j > 10$ — серьезная мультиколлинеарность

**Процедура анализа мультиколлинеарности:**

1. Рассчитать корреляционную матрицу признаков. Если есть коэффициенты корреляции близкие к $±1$, то мультиколлинеарность очевидна.
2. Если парные корреляции "нормальные", рассчитать VIF. Высокие значения VIF указывают на скрытую мультиколлинеарность.
3. При необходимости применить решения (регуляризация, отбор признаков).

@NB(`Парные корреляции между объясняющими переменными могут рассматриваться как достаточное, но ~~не необходимое условие мультиколлинеарности~~.`)

Если все парные коэффициенты корреляции меньше $1$, это не означает, что мультиколлиниарности нет. 

📋 **Пример. Парные корреляции vs Мультиколлинеарность.**

Рассмотрим объем Volume как функцию от обхвата Girth и высоты Height. Каждая точка данных характеризуется тремя признаками. Мультиколлинеарность означает, что точки данных лежат на одной плоскости: $$с_1 \dot Volume + c_2 \dot Girth + c_3 \dot Height = 0$$
Парные коэффициенты корреляции не связаны с этой плоскостью, они работают в проекциях. Для расчета парного коэффициента корреляции нужно спроектировать данные на соответсвующую плоскость признаков. А проекции этих данных могут принять какой угодно вид: может присутствовать рассеяние, коэффициент корреляции может быть любой.

@img04(plane1.png, Парные корреляции, 50)

----
### Регуляризованная регрессия
----

Регуляризация в регрессии
----

Если объясняющие переменные мультиколлинеарны, то обучение модели множественной регрессии является **некорректно поставленной** задачей, и МНК метод нестабилен.

При мультиколлинеарности матрица $X^TX$ становится плохо обусловленной. Это как попытка решить систему уравнений, где некоторые уравнения почти дублируют друг друга — решение становится неустойчивым к малым изменениям в данных.

Возможное решение — **регуляризовать** целевую функцию, чтобы сделать оптимальное решение единственным:
$$E'(\beta) = E(\beta) + r(\beta) = \frac{1}{n}\sum_{i=1}^n(y_i - x_i\beta)^2 + r(\beta)$$
где $r(\beta)$ — **регуляризатор** (не зависит от данных).

**Типы регуляризаторов:**

- $L_2$ регуляризатор (Ridge регрессия)
- $L_1$ регуляризатор (LASSO регрессия)

$L_2$ регуляризация (Ridge регрессия)
----

Модель линейной регрессии:
$$Y|x=x\beta+\varepsilon(x)$$

**$L_2$-регуляризованный критерий:**
$$E(\beta) = \sum_{i=1}^n(y_i - x_i\beta)^2 + \mu\sum_{j=0}^k\beta_j^2 = \sum_{i=1}^n(y_i - x_i\beta)^2 + \mu\beta^T\beta$$
где $\mu > 0$ — параметр регуляризации.

**Геометрическая интерпретация:** Регуляризатор $\mu\beta^T\beta$ представляет собой "чашу" с минимумом в нуле. Мы ищем компромисс между:

- Минимизацией ошибки на данных (первое слагаемое)
- Минимизацией "размера" коэффициентов (второе слагаемое)

**Система нормальных уравнений:**
$$(X^TX + \mu I)\beta = X^Ty$$

**МНК-оценки:**
$$\hat{\beta} = (X^TX + \mu I)^{-1}X^Ty$$

Даже если исходная матрица $X^TX$ была вырожденной, добавление $\mu I$ на главную диагональ делает её обратимой! Все собственные значения увеличиваются на $\mu > 0$.

Регрессия, обученная в соответствии с $L_2$-регуляризованным критерием, называется **Ridge регрессией**.

**Особенности Ridge регрессии:**

- Поскольку оценки $\hat{\beta}$ ~~смещены~~, статистические выводы обычно не рассматриваются, и предположение о нормальности остатков больше не требуется
- Параметр $\mu$ управляет компромиссом между смещением и дисперсией (bias-variance trade-oﬀ):

  - $\mu$ увеличивается ⇒ смещение увеличивается, дисперсия уменьшается
  - $\mu$ уменьшается ⇒ смещение уменьшается, дисперсия увеличивается

- Для больших значений $\mu$ функция регрессии становится приблизительно постоянной: $$\mu\gg 0\  \Rightarrow\  \varphi(x)\approx \bar y$$
- Регуляризация обычно не применяется к свободному члену $\beta_0$
- Кросс-валидация может использоваться для выбора $\mu$: выбирают значение $\mu$, которое выдает наименьшее значении ошибки кросс-валидации (cross-validation prediction error)

@quizQuestion(`Почему в Ridge регрессии обычно не регуляризуют свободный член $\beta_0$?`)

<!-- data-randomize -->
[( )] Это технически невозможно
[(x)] Свободный член не влияет на мультиколлинеарность
[( )] Это ухудшает качество модели
[( )] Свободный член всегда равен нулю

Свободный член $\beta_0$ может быть исключен из регрессионной модели с помощью центрирования и масштабирования:

$$Y'|x'=x'\beta'+\varepsilon'(x')$$
где $x'=(x'_1,...,x'_k)$, $\beta'=(\beta'_1,...,\beta'_k)^T$ и
$$x'_{ji}=\frac{x_{ji}-\bar x_j}{s_j}, \quad i=\overline{1,n}, \quad j=\overline{1,k}$$
$$y'_i=y_i-\bar y, \quad i=\overline{1,n}$$

Оригинальные коэффициенты:
$$\beta_0=\bar y-\sum\limits_{j=1}^k\frac{\beta'_j\bar x_j}{s_j}, \quad \beta_j=\frac{\beta'_j}{s_j}, \quad j=\overline{1,k}$$

После стандартизации все признаки имеют нулевое среднее и единичную дисперсию, что делает регуляризацию справедливой для всех коэффициентов. Коэффициенты $\beta'_1,...,\beta'_k$ сравнимы, что делает их полезными для визуального анализа "важности" регрессоров на графике ridge trace plot.

**График Ridge trace** - график оценок $\hat{\beta}'_1(\mu), ..., \hat{\beta}'_k(\mu)$ как функций параметра $\mu$ после центрирования и масштабирования.

@img04(ridge1.png, Ridge Trace Plot, 50)

С увеличением $\mu$ все коэффициенты монотонно стремятся к нулю, но ни один не обнуляется полностью.

$L_1$ регуляризация (LASSO регрессия)
----

Модель линейной регрессии:
$$Y|x=x\beta+\varepsilon(x)$$

**$L_1$-регуляризованный критерий:**
$$E(\beta) = \frac{1}{2}\sum_{i=1}^n(y_i - x_i\beta)^2 + \mu\sum_{j=0}^k|\beta_j|$$
где $\mu > 0$ — параметр регуляризации.

Регрессия, обученная с $L_1$-регуляризованным критерием, называется **LASSO (Least Absolute Shrinkage and Selection Operator) регрессия**.

**Геометрическое различие с Ridge:**
$L_1$ регуляризатор имеет форму "ромба" (в 2D) или "пирамиды" (в многомерном случае) с вершиной в нуле. Ключевое отличие:

- На осях координат штраф меньше, чем внутри квадрантов
- Алгоритму оптимизации "выгоднее" попасть на ось (занулить некоторые коэффициенты)

@img04(lasso1.jpg, L1vsL2, 60)

**Особенности LASSO:**

- ~~Не имеет решения в замкнутой форме~~ — требуются итерационные методы.
- $L_1$ регуляризация стремится концентрировать параметры регрессии в относительно небольшом числе высокозначимых параметров, в то время как другие приближаются к нулю.
- Полезна для ~~отбора признаков~~. В отличие от Ridge, LASSO может полностью обнулять коэффициенты. Это означает исключение соответствующих признаков из модели — автоматический feature selection.

**Trace plot для LASSO** показывает, как с увеличением параметра регуляризации коэффициенты последовательно обнуляются. Некоторые коэффициенты зануляются раньше, другие — позже. 
На графиках ниже можно сравнить Trace plot для Ridge и LASSO регрессии.

@img04(lasso2.png, Trace plot L1vsL2, 50)

**Сравнение $L_2$ и $L_1$ регуляризации:**

| Ridge ($L_2$) | LASSO ($L_1$) |
|---|---|
| Сжимает коэффициенты к нулю, но не обнуляет | Может обнулять коэффициенты |
| Включает все признаки | Автоматический отбор признаков |
| Имеет аналитическое решение | Требует численной оптимизации |

Elastic Net регрессия
----

Модель линейной регрессии:
$$Y|x=x\beta+\varepsilon(x)$$

**Elastic Net критерий:**
$$E(\beta) = \frac{1}{2}\sum_{i=1}^n(y_i - x_i\beta)^2 + \mu\sum_{j=0}^k\left(\frac{1-\alpha}{2}\beta_j^2 + \alpha|\beta_j|\right)$$
где:

- $\mu > 0$ — параметр регуляризации
- $\alpha$ — соотношение ridge/LASSO, $0 < \alpha < 1$

Оптимальные значения гиперпараметров $\mu$ и $\alpha$ обычно выбираются с помощью кросс-валидации по сетке параметров.

**Особенности Elastic Net:**

- Является комбинацией Ridge и LASSO регрессии
- Может генерировать обнуленные коэффициенты (как LASSO)
- Превосходит LASSO на данных с высоко коррелированными предикторами

@quizQuestion(`Сопоставьте значения $\alpha$, при которых Elastic Net превращается в Ridge/LASSO регрессию:`)

<!-- data-randomize -->
- [($\alpha = 0$) ($\alpha = 1$)]
- [    ( )           (x)]  LASSO
- [    (x)           ( )]  Ridge

----
### Отбор признаков
----

Отбор признаков для регрессии
----

**Отбор признаков** — это процесс выбора подмножества релевантных признаков для использования в построении модели.

**Зачем отбирать признаки?**

- Улучшение качества предсказания модели
- Обеспечение более быстрого обучения
- Обеспечение лучшей интерпретируемости обученной модели
 
@img04(cl_ex3.png, Feature Selection, 50)

Неправильный выбор признаков может привести к построению неадекватной модели. 
К этому также может привести неправильный выбор класса моделей (спецификация). **Неправильная спецификация модели** относится ко всем способам, которыми регрессионная модель может не соответствовать истинной базовой модели (или процессу генерации данных).

**Типы неправильной спецификации модели:**

1. **Недоспецификация (Underspecification)**. Пропущенные объясняющие переменные могут вызвать ~~смещение пропущенной переменной (omitted variable bias)~~ в МНК-оценках. Эффект неучтённых факторов "размазывается" по включённым переменным. Коэффициенты теряют интерпретируемость.
2. **Переспецификация (Overspecification)**. Избыточные объясняющие переменные могут вызвать ~~мультиколлинеарность~~ и неверные статистические выводы. Часть переменных может дублировать информацию. Ухудшаются статистические свойства оценок. Необходимо использовать регуляризаторы или делать предварительный отсев признаков.
3. **Неправильная спецификация функциональной формы (Functional form misspecification)**. Модель имеет соответствующие объясняющие переменные, но функциональная связь неверно специфицирована. Например, использование линейной модели для нелинейной зависимости

Для неправильно специфицированной модели предположения регрессионного анализа могут быть нарушены.

Добавление новой переменной в модель
----

Предположим, у нас есть регрессионная модель $Y|x = \beta_0 + \beta_1x + \varepsilon(x)$ и наблюдения для новой переменной $z$.
Следует ли включать эту переменную $z$ в модель? Первая мысль: "Давайте включим — хуже не будет!" Но это не всегда так.

**Анализ различных ситуаций:**

- **$z$ связана и с $x$, и с $y$**. Например, мы могли обнаружить эту связь по значению коэффициента корреляции или построить диаграммы рассеяния $x(z), y(z)$. Так как $z$ связана с $y$, то возможно она даст новую информацию при включению в модель. Однако, $z$ связана также с $x$, из-за чего может возникнуть мультиколлинеарность. В особенности это будет проблематично, если признаков много, тогда мультиколлинеарность обнаружить будет сложно и может быть включение $z$ того не стоит. Таким образом:

  - $z$ можно включить, но необходимо проверить на мультиколлинеарность, и в случае её обнаружения либо добавить регуляризатор, либо исключить $z$.

- **$z$ не связана с $x$, но связана с $y$**.

  - $z$ однозначно следует включить в модель, это уменьшит остаточную дисперсию

- **$z$ связана с $x$, но не связана с $y$**. Так как $z$ не связана с $y$, то никакой новой информации при включении в модель она не даст. Кроме того, связь z с x может привести к мультиколлинеарности.

  - $z$ не нужно включать в модель, т.к. это не только не уменьшит OVB и остаточную дисперсию, но и может вызвать мультиколлинеарность

- **$z$ не связана ни с $x$, ни с $y$**. Так как $z$ не связана с $x$, то её включение в модель не ухудшит свойств оценок, однако, вероятно, и не улучшит, так как $z$ не связана с $y$.

  - Не имеет большого значения, включать $z$ или исключать.

**Что происходит при добавлении новой переменной?**

После добавления $z$ возможны следующие сценарии:

- **Оценка $\hat{\beta}_1$ сильно изменяется**

  - $z$ связана с $x$ и $y$, её следует включить для избежания OVB, затем решить мультиколлинеарность

- **Оценка $\hat{\beta}_1$ не изменяется**: 

  - $z$ не связана с $x$ или не связана с $y$ (или с обеими)

    - Если $\mathrm{D}[\hat{\beta}_1]$ увеличивается → $z$ вызывает мультиколлинеарность, $z$ следует исключить
    - Если $\mathrm{D}[\hat{\beta}_1]$ уменьшается → $z$ и $y$ связаны. Независимо от того, что включение $z$ не уменьшит OVB, оно должно быть включено для уменьшения ошибок оценки
    - Если $\mathrm{D}[\hat{\beta}_1]$ не затронута → $z$ не связана с $x$ и $y$, $z$ бесполезна и её можно оставить или исключить

Подходы к отбору признаков
----

**Методы отбора признаков:**

1. **Фильтр-методы (Filter methods)**
Присваивают оценку каждому признаку, обычно одномерную и рассматривают признаки независимо. Отбор выполняется перед алгоритмом обучения. 
Примеры: Корреляция с целевой переменной, статистические тесты.

2. **Оберточные методы (Wrapper methods)**
Манипулируют признаками (добавляют или удаляют их) во время процесса обучения. Критерий отбора напрямую измеряет изменение в производительности модели в результате добавления или удаления признака. 
Примеры: Последовательный отбор признаков, пошаговая регрессия.

3. **Встроенные методы (Embedded methods)**
Изучают важность признаков как часть процесса обучения модели. После обучения получаем важность признаков в обученной модели.
Примеры: LASSO регрессия.

Последовательный отбор признаков
----

**Цель:** найти подмножество признаков, которое минимизирует ошибку предсказания.

Если число $k$ признаков мало, можно применить **исчерпывающий отбор признаков (exhaustive feature selection)** (перебрать все $2^k$ комбинаций), но это неосуществимо для большого $k$.

**Последовательный отбор признаков (Sequential feature selection)** — это разновидность ~~жадных поисковых алгоритмов~~.
Жадные алгоритмы не гарантируют глобального оптимума. Возможно, что на текущей итерации выбирается локально оптимальный признак вместе признака средней важности, но в комбинации с будущими признаками тот самый признак средней важности мог бы дать лучший результат. Последовательность локально оптимальных решений ≠ глобально оптимальное решение. Кроме того, для поиска оптимального подмножества из $k$ признаков нужно перебрать $\sum_{i=0}^k \binom{k}{i} = 2^k$ комбинаций.

Для последовательного отбора признаков нужны:

- ~~Целевая функция (критерий)~~ — мера качества модели и, следовательно, подмножества признаков (обычно MSE)
- ~~Поисковый алгоритм,~~ определяющий как добавлять или удалять признаки из подмножества

**Типы методов:**

- **Фильтр-методы:** критерий независим от процесса обучения
- **Оберточные методы:** критерий связан с моделью машинного обучения и функцией потерь, используемой при обучении

**Алгоритмы последовательного отбора:**

1. **Sequential forward selection (Последовательный прямой отбор).**
Наиболее значимые признаки последовательно добавляются к пустому множеству кандидатов до тех пор, пока добавление следующих признаков не приведет к снижению критерия.

2. **Sequential backward selection (Последовательный обратный отбор).**
Наименее значимые признаки последовательно удаляются из полного множества кандидатов до тех пор, пока удаление следующих признаков не приведет к увеличению критерия.

3. **Sequential forward floating selection (Последовательный прямой плавающий отбор).**
Признаки жадным образом последовательно добавляются, но также могут удаляться на некоторых шагах. Акцент делается на добавление признаков.

4. **Sequential backward floating selection (Последовательный обратный плавающий отбор)**
Признаки жадным образом последовательно удаляются, но также могут включаться на некоторых шагах. Акцент делается на удаление признаков.

Пошаговая регрессия
----

**Пошаговая регрессия (Stepwise regression)** — это разновидность последовательного плавающего отбора для регрессионной модели.

**Прямая пошаговая регрессия (Forward stepwise regression):** начинаем с простейшей модели.

**Обратная пошаговая регрессия (Backward stepwise regression):** начинаем со сложной модели со всеми возможными регрессорами.

**Алгоритм прямой пошаговой регрессии:**

**Шаг 1.** Обучение начальной модели с одним (или несколькими) наиболее значимым признаком.

**Шаг 2.** Добавление наиболее значимого признака в модель, если таковой имеется. Повторяем, пока есть значимые признаки для добавления.

**Шаг 3.** Удаление наименее значимого признака из модели, если таковой имеется, и переход к шагу 2. Если незначимых признаков нет, завершаем процесс.

@quizQuestion(`По аналогии с алгоритмом прямой пошаговой регрессии распишите самостоятельно алгоритм обратной пошаговой регрессии.`)

**Алгоритм обратной пошаговой регрессии:**<br><br>
**Шаг 1.** Обучение начальной модели [[с одним наименее значимым признаком | (со всеми признаками)]].<br><br>
**Шаг 2.** [[(Удаление) | Добавление]] [[наиболее | (наименее)]] значимого признака, если таковой имеется. Повторяем, пока есть [[значимые | (незначимые)]] признаки.<br><br>
**Шаг 3.** [[Удаление | (Добавление)]] [[(наиболее) | наименее]] значимого признака, если таковой имеется, и переход к шагу 2. Если [[(значимых)|незначимых]] признаков нет, завершаем процесс.

**Критерий значимости:**
Мера значимости признака обычно основана на степени влияния на коэффициент детерминации $R^2$ и проверяется с помощью $F$-теста.

Предположим, у нас есть два набора признаков $𝒳_0$ и $𝒳_1$ и соответствующие обученные регрессионные модели.

**Статистическая гипотеза:** $H_0: R^2_1 = R^2_0$ (модели объясняют данные одинаково хорошо)

**Тестовая статистика:**
$Z = \frac{(D^*_{res0} - D^*_{res1})/(k_1 - k_0)}{D^*_{res1}/(n - k_1 - 1)}, \quad Z|H_0 \sim F(k_1 - k_0, n - k_1 - 1)$
где $k_0, k_1$ — числа объясняющих переменных в моделях, $D^*_{res0}, D^*_{res1}$ — остаточные дисперсии моделей:
$$D^*_{res} = \frac{1}{n}\sum_{i=1}^n(y_i - \varphi(x_i, \beta))^2$$

**Принятие решения:**
$p > \alpha \Rightarrow H_0$ принимается, различие незначимо (признак не нужен)
$p \leq \alpha \Rightarrow H_0$ отвергается, различие значимо (признак полезен)

**Практическое применение:**
В пошаговой регрессии $p$-value от $F$-теста используется как мера значимости признака: меньший $p$-value = более значимый признак. Можно ранжировать признаки по убыванию значимости.

**Ограничения пошаговой регрессии:**

- Пошаговый подход может использоваться с OLS, WLS, GLS. Для робастных регрессий $F$-тест больше не валиден.
- Рекомендуется проверить и удалить выбросы перед пошаговой регрессией, т.к. выбросы могут исказить оценки значимости признаков.
- Пошаговая регрессия находит субоптимальное подмножество признаков. Глобальный оптимум целевой функции не гарантирован, т.к. жадная природа алгоритма может пропустить лучшие комбинации.
- Пошаговый отбор использует много повторных тестов гипотез для принятия решений о включении или исключении отдельных предикторов. Это приводит к увеличению числа ложноположительных результатов (~~inflation of false positive findings~~).
- Следует использовать валидационную выборку для оценки качества подмножества признаков на каждой итерации.
- Пошаговая регрессия вызывает много критики [в литературе](https://doi.org/10.1007/978-3-319-19425-7).

📋 **Пример результата пошаговой регрессии:**

@img04(step1.png, Stepwise Regression Example, 70)

----
### ❓ Вопросы для самопроверки
----

1. Что такое регрессионная диагностика и каковы её основные задачи?
2. Перечислите основные предположения линейной регрессии и объясните последствия их нарушения.
3. Как графики остатков помогают выявить нарушения предположений регрессии?
4. Что такое смещение пропущенной переменной (OVB) и как оно возникает?
5. Объясните, что показывает частичный регрессионный график и как его интерпретировать.
6. Как Q-Q график помогает проверить нормальность остатков? Какие паттерны указывают на нарушения?
7. В чём различие между выбросами, точками с высоким leverage и влиятельными наблюдениями?
8. Что измеряет расстояние Кука и как его интерпретировать?
9. Что такое мультиколлинеарность и какие проблемы она вызывает?
10. Как работает фактор инфляции дисперсии (VIF) для диагностики мультиколлинеарности?
11. В чём различие между $L_1$ и $L_2$ регуляризацией? Когда использовать каждую?
12. Объясните основные подходы к отбору признаков: фильтровые, оберточные и встроенные методы.
13. Как работает пошаговая регрессия?

----
### ✅ Тест по разделу
----

@quizQuestion(`Что является основной целью регрессионной диагностики?`)

<!-- data-randomize -->
[( )] Построение новых моделей регрессии
[(x)] Проверка согласованности модели и предположений с наблюдаемыми данными
[( )] Увеличение точности предсказаний
[( )] Сокращение количества переменных в модели

@quizQuestion(`Какое предположение нарушается при наличии смещения пропущенной переменной (OVB)?`)

<!-- data-randomize -->
[( )] Линейность
[( )] Гомоскедастичность
[(x)] Экзогенность регрессоров
[( )] Нормальность остатков

@quizQuestion(`Что показывает частичный регрессионный график?`)

<!-- data-randomize -->
[( )] Корреляцию между всеми переменными
[(x)] Связь между переменными при исключении влияния других переменных
[( )] Распределение остатков модели
[( )] Наличие выбросов в данных

@quizQuestion(`Какой паттерн на Q-Q графике указывает на правостороннюю асимметрию данных?`)

<!-- data-randomize -->
[(x)] Кривая выпукла вниз
[( )] Кривая выпукла вверх
[( )] S-образная кривая
[( )] Прямая линия

@quizQuestion(`Что измеряет расстояние Кука?`)

<!-- data-randomize -->
[( )] Корреляцию между переменными
[( )] Нормальность распределения остатков
[(x)] Влияние наблюдения на предсказанные значения отклика
[( )] Линейность связи между переменными

@quizQuestion(`Какая регуляризация может обнулять коэффициенты модели?`)

<!-- data-randomize -->
[( )] L2 регуляризация (Ridge)
[(x)] L1 регуляризация (LASSO)
[( )] Обе регуляризации
[( )] Ни одна из регуляризаций

@quizQuestion(`Какие проблемы может вызвать мультиколлинеарность?`)

<!-- data-randomize -->
[[x]] Широкие доверительные интервалы
[[x]] Низкие t-статистики
[[x]] Нестабильность оценок коэффициентов
[[ ]] Нарушение нормальности остатков
[[ ]] Гетероскедастичность

@quizQuestion(`Какой тест используется для сравнения моделей в пошаговой регрессии?`)

<!-- data-randomize -->
[(x)] F-тест
[( )] t-тест
[( )] Тест хи-квадрат
[( )] Тест Дарбина-Уотсона

@quizQuestion(`Если на частичном регрессионном графике подогнанная линия горизонтальна, это означает:`)

<!-- data-randomize -->
[( )] Переменную обязательно нужно включить в модель
[(x)] Переменная не содержит дополнительной информации для предсказания
[( )] Есть сильная нелинейная зависимость
[( )] Присутствует мультиколлинеарность

@quizQuestion(`Рассмотрим пример: зарплата зависит от образования и опыта. Если мы исключим опыт из модели, а образование и опыт положительно коррелируют, то коэффициент при образовании будет:`)

<!-- data-randomize -->
[( )] Недооценен (отрицательное смещение)
[(x)] Переоценен (положительное смещение)
[( )] Несмещен
[( )] Равен нулю

@quizQuestion(`Какая форма на графике остатков характерна для гетероскедастичности?`)

<!-- data-randomize -->
[[x]] Воронкообразная форма
[[x]] Обратная воронка
[[ ]] Случайное рассеяние
[[x]] Систематическое изменение разброса остатков
[[ ]] Горизонтальная полоса

@quizQuestion(`На Lag Plot точки образуют линейную зависимость. Это означает:`)

<!-- data-randomize -->
[( )] Остатки независимы
[(x)] Есть сильная автокорреляция остатков
[( )] Модель правильно специфицирована
[( )] Нет гетероскедастичности

@quizQuestion(`Чем отличается выброс от влиятельного наблюдения?`)

<!-- data-randomize -->
[( )] Ничем, это синонимы
[(x)] Выброс имеет большой остаток, влиятельное наблюдение сильно изменяет коэффициенты
[( )] Влиятельное наблюдение всегда является выбросом
[( )] Выброс всегда влияет на коэффициенты

@quizQuestion(`Расстояние Кука $D_i > 1$ означает:`)

<!-- data-randomize -->
[( )] Наблюдение является выбросом
[(x)] Наблюдение сильно влияет на регрессию
[( )] Есть мультиколлинеарность
[( )] Нарушена нормальность

@quizQuestion(`Парные корреляции между регрессорами близки к нулю, но $VIF$ некоторых переменных очень высокие. Это означает:`)

<!-- data-randomize -->
[( )] Ошибка в расчетах VIF
[(x)] Есть скрытая мультиколлинеарность в многомерном пространстве
[( )] Мультиколлинеарности нет
[( )] Нужно пересчитать корреляции

@quizQuestion(`В Ridge регрессии параметр $μ$ контролирует:`)

<!-- data-randomize -->
[( )] Количество итераций обучения
[(x)] Компромисс между смещением и дисперсией
[( )] Скорость сходимости алгоритма
[( )] Размер выборки

@quizQuestion(`На графике Ridge trace с увеличением $μ$ коэффициенты:`)

<!-- data-randomize -->
[(x)] Монотонно стремятся к нулю, но не обнуляются
[( )] Остаются постоянными
[( )] Увеличиваются по модулю
[( )] Обнуляются поочередно

@quizQuestion(`Какие проблемы может решить логарифмирование переменных?`)

<!-- data-randomize -->
[[x]] Гетероскедастичность
[[x]] Нелинейность
[[x]] Выбросы
[[ ]] Мультиколлинеарность
[[x]] Нарушение нормальности


