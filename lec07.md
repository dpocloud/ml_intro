<!--

author:     Alexander Trofimov
attribute:  **Editor:** Daria Gudkova
email:    
date:       21/02/2025
version:    0.1
language:   en
narrator:   English

icon:    ./icon.png
logo:    ./course_logo.png

comment:  Курс "Введение в машинное обучение"

link:     ./custom.css

import:   ./macro.md

-->


## Логистическая регрессия
-----

!?[Lecture07 Video](https://www.youtube.com/watch?v=8pTEmbeENF4)

---
<h3 style="text-align: center;">Содержание раздела</h3>

- Понятие функции связи (link function). 

  - Виды функций связи, их свойства. 
  - Логит и пробит-функции связи. 
  - Логистическая функция. 

- Бинарная логистическая регрессия. 

  - Логистическая модель апостериорных вероятностей классов. 
  - Постановка задачи обучения бинарной логистической регрессии. 
  - Критерий обучения и метод обучения. 
  - Логистическая функция потерь. 
  - Регуляризованная логистическая регрессия. 

- Многоклассовая логистическая регрессия. 

  - Softmax-модель апостериорных вероятностей классов. 
  - Постановка задачи обучения многоклассовой логистической регрессии. 
  - Критерий обучения и метод обучения.


----
### Бинарная классификация
----

Постановка задачи
----

**Дано:**

$D=\{(x^{(1)},y^{(1)}),...,(x^{(n)},y^{(n)})\}$ -- обучающая выборка,

$(x^{(i)},y^{(i)})\in X\times Y,\hspace{5mm}i=\overline{1,n}$

$X=\mathbb{R}^M$ -- множество признаков,

$Y=\{-1,1\}$ -- множество меток.

Пусть $y$ --  результат применения неизвестного отображения

$F:X\rightarrow Y,\hspace{5mm}y^{(i)}=F(x^{(i)}),\hspace{5mm}i=\overline{1,n}$

**Требуется:**

Построить гипотезу $h\in H,\ H=\{h:h(x),h(x)\in Y\}$, 
аппроксимирующую неизвестное отображение $F$

**Предположение:**

Класс гипотез $H=\{h:h(x)=sign\varphi(x,w)\}$,
где $\varphi(x,w)\in\mathbb{R}$ -- классификационные очки объекта $x\in X$ , $w\in\mathbb{R}^L$ -- вектор параметров

Классификационные очки $\varphi(x,w)$ можно интерпретировать как "уверенность" классификатора:

- Большие положительные значения → высокая уверенность в положительном классе
- Большие отрицательные значения → высокая уверенность в отрицательном классе
- Значения близкие к нулю → низкая уверенность, объект близко к границе

Линейный классификатор
----

Вектор признаков $x=(x_0,x_1,...,x_M)^T\mathbb{R}^{M+1},\ x_0=1$

**Линейный классификатор** -- классификатор, формирующий решение $h(x)$ на основе значения линейной комбинации признаков $x_0,...,X_M$

**Классификационные очки объекта $x\in X$:**
$$\varphi(x,w)=w_0+w_1x_1+...+w_Mx_M=w^Tx$$
где $w=(w_0,w_1,...,w_M)^T$ -- вектор параметров

**Зачем добавлять $x_0 = 1$?**
Мы добавляем фиктивный признак $x_0 = 1$ для того, чтобы формулы выглядели красивее. Линейная форма обычно включает свободный член $w_0$, и чтобы не таскать его отдельно во всех формулах, мы включаем его в векторное произведение $w^Tx$.

**Класс гипотез:**

$H=\{h:h(x)=sign(w_0+w_1x_1+...+w_Mx_M)\}$

**Пример:**

Нормальный байесовский классификатор с разделяемой (shared) ковариационной матрицей: когда ковариационная матрица одинакова для всех классов, квадратичная часть в дискриминантных функциях сокращается, и решение принимается только на основе линейной комбинации признаков.

Вероятностный классификатор
----

**Вероятностный классификатор** -- классификатор, способный вычислять не только метку класса, но и вероятности принадлежности вектора признаков $x_0,...,x_M$ всем классам.

Вероятностные классификаторы дают дополнительную информацию о "уверенности" в предсказании. Например, предсказание с вероятностью 0.51 менее надежно, чем с вероятностью 0.95.

**Примеры вероятностных классификаторов:**

Байесовский классификатор, логистическая регрессия

**Примеры не-вероятностных классификаторов:**

Машина опорных векторов (SVM), LDA

Некоторые не-вероятностные классификаторы могут быть модифицированы, чтобы вычислять вероятности (например, машина опорных векторов)

~~Логистическая регрессия -- линейный вероятностный классификатор~~

----
#### Функция связи
----

Могут ли классификационные очки $\varphi(x,w)$ быть использованы для оценки вероятности $P(Y = 1|x)$?

**Интуитивные соображения:**

Рассмотрим различные значения классификационных очков:

- $\varphi(x,w) = 0.01$: Формально знак положительный (класс $+1$), но мы понимаем, что это очень ненадежный результат. Точка близко к границе, вероятность должна быть около $0.5$.
- $\varphi(x,w) = 0$: Точно на границе между классами, вероятность должна быть $0.5$.
- $\varphi(x,w) = 500$: Классификатор очень уверен в положительном классе, вероятность должна быть близка к $1$.
- $\varphi(x,w) = -500$: Классификатор очень уверен в отрицательном классе, вероятность должна быть близка к $0$.

**Формальная связь:**

- Чем выше $\varphi(x,w)$, тем больше вероятность $P(Y = 1|x)$ 
- Чем ниже $\varphi(x,w)$, тем больше вероятность $P(Y = −1|x)$
$\varphi(x,w)=0\Rightarrow P(Y=-1|x)=P(Y=1|x)=0.5$

Чтобы использовать $\varphi(x,w)\in(-\infty,+\infty)$ как меру вероятности $P(Y = 1|x)$, необходимо отобразить их монотонно на $[0; 1]$

**Функция связи (link function)** -- это функция $f:[0;1]\rightarrow\mathbb{R}$, определяющая связь между вероятностями $P (Y = 1|x)$ и классификационными очками $\varphi(x,w)$:
$$
\begin{gather}
\varphi(x,w)=f(P(Y=1|x,w))
\\
P(Y=1|x,w)=f^{-1}(\varphi(x,w))
\end{gather}
$$

**Требования к функции связи:**

- Монотонность: большие очки → большие вероятности
- Гладкость: для удобства оптимизации
- $f^{-1}(0) = 0.5$: нулевые очки соответствуют равным вероятностям классов

~~Сигмоидные функции~~ удовлетворяют этим требованиям -- они монотонно возрастающие, ограниченные, непрерывные и дифференцируемые. Наиболее известными примерами сигмоидной функции являются ~~логистическая функция~~ и ~~функция распределения стандартизованного нормального распределения~~.

Логит-функция
----

$$P(Y=1|x,w)=p,\hspace{5mm}P(Y=-1|x,w)=1-p$$

~~Логит-функция связи~~: $f(p)=logit(p)=\ln(\frac{p}{1-p})$

~~Обратная логит-функция~~: $f^{-1}(\varphi)=logistic(\varphi)=\frac{1}{1+e^{-\varphi}}$

Обратная логит-функция -- ~~логистическая функция~~

@img07(logistic.png, Логистическая функция, 70)

Отношение $\frac{p}{1-p}=\frac{P(Y=1|x,w)}{P(Y=-1|x,w)}$ называется **отношением шансов (odds ratio)**

- Если $p = 0.5$, то отношение шансов $= 1$, логарифм $= 0$
- Если $p > 0.5$, то отношение шансов $> 1$, логарифм $> 0$
- Если $p < 0.5$, то отношение шансов $< 1$, логарифм $< 0$

**Интерпретация логит-функции связи**:

Классификационные очки $\varphi(x, w)$ могут быть интерпретированы как отношение шансов для вектора признаков $x ∈ 𝒳$

Пробит-функция
----

$$P(Y=1|x,w)=p,\hspace{5mm}P(Y=-1|x,w)=1-p$$

~~Пробит-функция связи~~: $f(p)=probit(p)=\Phi^{-1}(p)$

~~Обратная функция~~: $f^{-1}(\varphi)=\Phi(\varphi)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\varphi\exp(-\frac{u^2}{2})du$

Обратная пробит-функция -- ~~функция распределения $\Phi$ стандартизованного нормального распределения~~.

@img07(gaussian.png, Обратная пробит-функция, 70)

Логит-функция vs пробит-функция
----

Логистическая функция более пологая, чем функция распределения стандартизованного нормального распределения.

| Логит-функция | Пробит-функция |
|---|---|
| Хороший аналитический вид: легко вычислять и дифференцировать | Сложный аналитический вид: требует вычисления интеграла, который не выражается в элементарных функциях, необходимы табличные значения или численные методы |
| Понятная интерпретация: классификационные очки как логарифм отношения шансов | Нет понятной интерпретации |

На практике чаще используется логит-функция из-за её вычислительных преимуществ и лучшей интерпретируемости.

Логистическая модель апостериорных вероятностей классов
----

Линейная модель для классификационных очков:
$$\varphi(x,w)=w^Tx$$

Логит-функция связи приводит к ~~линейной модели для отношения шансов~~:
$$\varphi(x,w)=logit(p)=\ln\frac{p}{1-p}=w^Tx$$

**Для заданного $x$ апостериорная вероятность $p$ позитивного класса:**
$$p=logistic(\varphi(x,w))=\frac{1}{1+e^{-\varphi(x,w)}}=\frac{1}{1+e^{-w^Tx}}$$

Обратная логит-функция -- логистическая функция.

Логит-функция связи приводит к ~~логистической модели апостериорных вероятностей классов~~.

---

**Ключевая идея логистической регрессии:**

Сделав два предположения:

- Классификатор линейный: $\varphi(x,w) = w^Tx$
- Функция связи -- логит: $\varphi(x,w) = \ln\frac{p}{1-p}$

Мы получили модель для расчета апостериорных вероятностей классов:
$P(Y=1∣x)=\frac{1}{1+e^{-w^Tx}}$. 

**Важный вопрос:** Почему это будут именно вероятности, а не просто какие-то числа от $0$ до $1$?

Пока что мы просто предположили такую модель. Обоснование того, что это действительно вероятности, мы получим позже через статистическую интерпретацию и связь с байесовской классификацией.

----
#### Отношение шансов 
----

Отношение шансов для байесовского классификатора
----

Давайте проанализируем байесовский классификатор с точки зрения отношения шансов и покажем, откуда в нём может появиться логистическая функция.

**Обозначения:**

$P (Y = k|𝑋 = x) = p_Y (k|x)$ -- апостериорная вероятность метки $k$ 

$P (Y = k) = p_Y (k)$ -- априорная вероятность метки $k$  

$P (𝑋 = x|Y = k) = p_𝑋 (x|k)$ -- правдоподобие класса $k$

$P (𝑋 = x) = p_𝑋 (x)$ -- маргинальная вероятность $x$  

$p = p_Y (+1|x)$ -- апостериорная вероятность метки $+1$  

$1 − p = p_Y (−1|x)$ -- апостериорная вероятность метки $–1$

**Теорема Байеса:**
$$p_Y(k|x)=\frac{p_X(x|k)p_Y(k)}{p_X(x)}$$

**Логарифмическое отношение шансов:**
$$\ln\frac{p}{1-p}=\ln\frac{p_Y(+1|x)}{p_Y(-1|x)}=\ln\frac{p_X(x|_{+1})p_Y(+1)}{p_X(x|_{-1})p_Y(-1)}$$

**Для нормального наивного байесовского классификатора с разделяемой ковариационной матрицей:**
$$p_X(x|k)=\prod_{j=1}^Mp_{X_j}(x_j|k)=\prod_{j=1}^M\frac{1}{\sigma_j\sqrt{2\pi}}\exp(-\frac{(x_j-m_{j|k})^2}{2\sigma_j^2})$$

где $m_{j|k}$ -- математическое ожидание $j$-го признака в $k$-том классе, $\sigma_j$ - стандартное отклонение $j$-го признака (одинаковое для всех классов, т.к. матрица разделяемая).

**Логарифмическое отношение шансов:**
$$\ln\frac{p}{1-p}=...=w_0+\sum_{j=1}^Mw_jx_j$$

где

$w_0=\ln\frac{p_Y(+1)}{p_Y(-1)}+\sum_{j=1}^M\frac{m_{j|-1}^2-m_{j|+1}^2}{2\sigma_j^2}$

$w_j=\frac{m_{j|-1}^2-m_{j|+1}^2}{2\sigma_j^2},\hspace{5mm}j=\overline{1,M}$

Логистическая регрессия и байесовская классификация
----

Лог. отношение шансов для нормального наивного байесовского классификатора с разделяемой ков. матрицей является ~~линейным~~:
$$\ln\frac{p}{1-p}=w_0+\sum_{j=1}^Mw_jx_j=w^Tx$$

Это приводит к ~~логистической модели апостериорных вероятностей классов~~:
$$p=logistic(w^Tx)=\frac{1}{1+e^{-w^Tx}}$$

В предположении, что признаки $x_1, ..., x_M$ независимы и нормально распределены с разделяемой ковариационной матрицей, параметры $w_0,...,w_M$ логистической регрессии могут быть рассчитаны аналитически.

Значение $w^Tx$ -- классификационные очки $\varphi(x)$ примера $x\in X$

@quizQuestion(`При каких условиях байесовский классификатор эквивалентен логистической регрессии?`)

- [( )] При любых условиях
- [( )] Только при нормальном распределении
- [(X)] При нормальном наивном байесовском классификаторе с разделяемой ковариационной матрицей
- [( )] Никогда не эквивалентен

----
#### Обучение логистической регрессии
----

Дискриминативный подход: логистическая регрессия
----

$$\varphi(x,w)=logit(p(x,w))=\ln\frac{p(x,w)}{1-p(x,w)}=w^Tx$$
$$p(x,w)=P(Y=1|x,w)=logistic(\varphi(x,w))=\frac{1}{1+e^{-w^Tx}}$$

Как оценить параметры $w_0,...,w_M$ по данным без каких-либо предположений о распределениях?

Дискриминативный подход состоит в аппроксимации классификационных очков $\varphi(x,w)$ без оценивания распределения $f_{XY}(x,y)$

**Логистическая регрессия:** моделируем апостериорные вероятности классов $p(x, w)$, связанные с классификационными очками $\varphi(x,w)$ через логит-функцию связи.

Мы знаем, что логистическая модель теоретически обоснована для определенных условий. Но вместо построения статистической модели данных, мы просто запустим процедуру оптимизации. Не важно, нормальное ли распределение, наивный ли классификатор. Мы запустим оптимизацию и найдем наиболее подходящие параметры для конкретных данных (такие веса $w$, которые минимизируют ошибку на обучающих данных).

Логистическая модель классов
----

Чтобы процедура оптимизации имела статистический смысл, введем статистическую модель.

$D=\{(x^{(1)},y^{(1)}),...,(x^{(n)},y^{(n)})\}$ -- обучающая выборка

Для удобства записи распределения Бернулли переразметим: $y^{(i)}:=\frac{y^{(i)}+1}{2},\hspace{5mm}i=\overline{1,n}$,     тогда  $y\in \{0,1\}$

Предположим, что $y^{(i)}$ получено из **распределения Бернулли**: $Y_i\sim B(1,p(x_i,w))$, где

- $p(x^{(i)},w)=P(Y_i=1|x^{(i)},w)$ -- вероятность "успеха" (метки $1$)
- $P(Y_i=0|x^{(i)},w)=1-p(x^{(i)},w)$ -- вероятность "неудачи" (метки $0$)

**Интерпретация:** Мы считаем, что метка класса -- это результат "подбрасывания монетки", но вероятность выпадения зависит от признаков объекта $x$. Для одних объектов $p(x,w) \approx 1$ (почти всегда класс $1$), для других $p(x,w) \approx 0$ (почти всегда класс $0$).

**Общая формула распределения Бернулли:**

$P(Y_i=k|x^{(i)},w)=p(x^{(i)},w)^k(1-p(x^{(i)},w))^{1-k}$

Для оценивания вектора параметров $w$ используем **метод максимального правдоподобия**.

**Функция правдоподобия:**

$$L(y^{(1)},...,y^{(n)},w)=\prod_{i=1}^np(x^{(i)},w)^{y^{(i)}}(1-p(x^{(i)},w))^{1-y^{(i)}}\rightarrow\max_w$$

Мы ищем такие параметры $w$, при которых наблюдаемые данные имели бы максимальную вероятность появления. Если для примера с меткой $1$ наша модель предсказывает $p(x,w) = 0.9$, это хорошо. Если предсказывает $p(x,w) = 0.1$, это плохо.

**Лог-функция правдоподобия:**

$$E(w)=-\sum_{i=1}^n(y^{(i)}\ln p(x^{(i)},w)+(1-y^{(i)})\ln(1-p(x^{(i)},w)))\rightarrow\min_w$$

**Логистическая модель для $p(x, w)$:**

$$p(x,w)=logistic(w^Tx)=\frac{1}{1+e^{-w^Tx}}$$

Теперь у нас есть полное статистическое обоснование! Минимизируя логарифм функции правдоподобия, мы действительно ищем вероятности в рамках корректной статистической модели.

Из-за нелинейности сигмоидной функции, аналитически решить задачу оптимизации сложно, поэтому используется **градиентный спуск**:

$$w(t+1)=w(t)-\eta\frac{\partial E(t)}{\partial w},\hspace{5mm}w(0)=w^0$$

где $\frac{\partial E(t)}{\partial w}$ -- градиент, $𝜂 > 0$ -- параметр метода

**Алгоритм градиентного спуска:**

1. **Инициализация**: Выбираем начальную точку $w^{(0)}$
2. **Итерации**:

   - Вычисляем градиент в текущей точке
   - Делаем шаг в направлении антиградиента
   - Повторяем до сходимости

**Почему нужно делать маленькие шаги?** Если шаг слишком большой, мы можем "перепрыгнуть" минимум или даже попасть в область, где функция возрастает.

**Производные логистической функции:**

$$p=logistic(\varphi)=\frac{1}{1+e^{-\varphi}},\hspace{5mm}\frac{dp}{d\varphi}=p(1-p)$$

Производная логистической функции выражается через саму функцию! Это делает вычисления очень удобными.

**Производные лог-функции правдоподобия:**

$$\frac{\partial E}{\partial w_j}=\sum_{i=1}^n(p(x^{(i)},w)-y^{(i)})x_j^{(i)}$$

**Интерпретация градиента:** 

Для каждого обучающего примера $(x^{(i)}, y^{(i)})$:

- $p(x^{(i)},w)$ -- предсказанная вероятность
- $y^{(i)}$ -- истинная метка
- $(p(x^{(i)},w) - y^{(i)})$ -- ошибка предсказания

Градиент представляет собой сумму взвешенных ошибок по всем примерам.

**Процедура обучения:**

1. Подаем входной пример $x^{(i)}$ на модель
2. Получаем предсказанную вероятность $p(x^{(i)},w)$  
3. Сравниваем с истинной меткой $y^{(i)}$
4. Вычисляем ошибку и обновляем веса
5. Повторяем для всех примеров

**Критерий останова:** 
Обучение продолжается до тех пор, пока не выполнится одно из следующих условий:

- Градиент станет достаточно маленьким
- Изменение функции потерь станет пренебрежимо малым
- Будет достигнуто максимальное число итераций

ERM-принцип для логистической регрессии
----

Теперь покажем связь с принципом минимизации эмпирического риска (ERM).

Пусть $Y =\{−1,1\}$ (возвращаемся к исходной разметке)

**ERM-принцип:**

$$R^*(w)=\sum_{i=1}^nL(m((x^{(i)},y^{(i)}),w))\rightarrow\min_w$$

где $m((x^{(i)},y^{(i)}),w)=y^{(i)}\varphi(x^{(i)})=y^{(i)}w^Tx^{(i)}$ -- **классификационный отступ (margin)** примера $x^{(i)},\ i=\overline{1,n}$

**Margin** показывает, насколько "правильно" и "уверенно" классификатор принял решение:

- Положительный margin: Знак очков совпадает с истинной меткой (правильная классификация)
- Отрицательный margin: Знак очков не совпадает с истинной меткой (ошибка)
- Большой по модулю margin: Высокая уверенность классификатора
- Малый по модулю margin: Низкая уверенность, объект близко к границе

**Логистическая функция потерь:**
$$L(m)=\ln(1+e^{-m})$$

**Эмпирический риск:**
$$R^*(w)=\sum_{i=1}^n\ln(1+e^{-y^{(i)}w^Tx^{(i)}})$$

**Производные:**
$$\frac{\partial R^*}{\partial w_j}=\sum_{i=1}^n\frac{1}{1+e^{-y^{(i)}w^Tx^{(i)}}}e^{-y^{(i)}w^Tx^{(i)}}(-y^{(i)}x_j^{(i)})$$

**Для $y^{(i)}=-1$:**
$$\frac{\partial R^*}{\partial w_j}=...=\sum_{i=1}^np(x^{(i)},w)x_j^{(i)}$$

**Для $y^{(i)}=1$:**
$$\frac{\partial R^*}{\partial w_j}=...=\sum_{i=1}^n(p(x^{(i)},w)-1)x_j^{(i)}$$

**Производные эмпирического риска $(Y=\{-1,1\})$:**
$$
\frac{\partial R^*}{\partial w_j}=
\begin{cases}
\sum_{i=1}^np(x^{(i)},w)x_j^{(i)},\hspace{5mm}y^{(i)}=-1
\\
\sum_{i=1}^n(p(x^{(i)},w)-1)x_j^{(i)},\hspace{5mm}y^{(i)}=1
\end{cases}
$$

**Производные лог-функции правдоподобия $(Y=\{0,1\})$:**
$$
\frac{\partial E}{\partial w_j}=\sum_{i=1}^n(p(x^{(i)},w)-y^{(i)})x_j^{(i)}=
\begin{cases}
\sum_{i=1}^np(x^{(i)},w)x_j^{(i)},\hspace{5mm}y^{(i)}=0
\\
\sum_{i=1}^n(p(x^{(i)},w)-1)x_j^{(i)},\hspace{5mm}y^{(i)}=1
\end{cases}
$$

@NB(`Задача обучения логистической регрессии -- это задача минимизации эмпирического риска с логистической функцией потерь.`)

Регуляризованная логистическая регрессия
----

Предположение об априорном распределении параметров $f_W(w)$ приводит к добавлению регуляризатора $r(w)=-\ln f_W(w)$ в критерий обучения (см. предыдущие темы)

**Регуляризованный эмпирический риск:** 
$$R'(h)=R^*(h)+r(w)$$

Предположение о независимых и нормально распределенных параметрах с нулевым мат. ожиданием и дисперсиями $\sigma^2$ приводит к $L_2–$регуляризатору:
$$r(w)=-\ln f_W(w)=\frac{1}{2\sigma^2}\sum_{j=1}^Mw_j^2+const$$

**Производные $(Y=\{-1,1\})$:**
$$
\frac{\partial R'}{\partial w_j}=\frac{\partial R^*}{\partial w_j}+\frac{1}{\sigma^2}w_j,\hspace{5mm}j=\overline{1,M}
$$

----
### Многоклассовая классификация
----

Постановка задачи
----

**Дано:**

$D=\{(x^{(1)},y^{(1)}),...,(x^{(n)},y^{(n)})\}$ -- обучающая выборка,

$(x^{(i)},y^{(i)})\in X\times Y,\hspace{5mm}i=\overline{1,n}$

$X=\mathbb{R}^M$ -- множество признаков,

$Y=\{1,...,K\}$ -- множество меток  

**Предположение:**

Класс гипотез $H=\{h:h(x)=\arg\max_{k=\overline{1,L}}\varphi(x,w_k)\}$

где:

- $\varphi(x,w_k)\in\mathbb{R}$ -- ~~классификационные очки~~ объекта x в пользу $k$-го класса, $k=\overline{1,K}$,  

- $x=(x_0,x_1,...,x_M)^T\in\mathbb{R}^{M+1},\ x_0=1$

- $w_k\in\mathbb{R}^{M+1}$ -- ~~вектор параметров~~, связанный с $k$-м классом, $k=\overline{1,K}$

Классификационные очки в пользу каждого класса ~~линейны~~:
$$\varphi(x,w_k)=w_{0k}+w_{1k}x_1+...+w_{Mk}x_M=w_k^Tx$$

В отличие от бинарной классификации, где у нас была одна дискриминантная функция $\varphi(x,w)$, теперь у нас есть $K$ функций -- по одной для каждого класса. Классификатор работает по принципу "победитель получает всё": вычисляются классификационные очки для всех классов, и выбирается класс с максимальными очками.

----
#### Softmax-функция
----

Связь с апостериорными вероятностями
----

Чем больше $\varphi(x,w_k)$, тем выше вероятность $P(Y = k|x)$
$$\varphi(x,w_{k^*})=\max_{k=\overline{1,K}}\varphi(x,w_k)\Rightarrow P(Y=k^*)=\max_{k=\overline{1,K}}P(Y=k|x)$$

У нас есть $K$ значений $\varphi(x,w_1),...,\varphi(x,w_K)$, каждое из которых может принимать значения от $-\infty$ до $+\infty$. Чтобы интерпретировать их как меру вероятностей $P (Y = 1|x), ..., P (Y = K |x)$, нужно:

1. Отобразить все значения на интервал $[0,1]$
2. Обеспечить, чтобы сумма всех значений равнялась $1$: $\sum_{k=1}^KP(Y=k|x)=1$
3. Сохранить монотонность: большие очки → большие вероятности

Для такого отображения используется **softmax-функция**: 
$$(p_1,...,p_K)=softmax(\varphi(x,w_1),...,\varphi(x,w_K))$$
$$p_k=P(Y=k|x),\hspace{5mm}\varphi(x,w_k)=w_k^Tx,\hspace{5mm}k=\overline{1,K}$$

Softmax-функция
----

**Softmax-функция** -- это функция $\mathbb{R}^K\rightarrow[0;1]^K$ вида:
$$(p_1,...,p_K)=softmax(\varphi_1,...,\varphi_K)\Leftrightarrow p_k=\frac{e^{\varphi_k}}{\sum_{i=1}^Ke^{\varphi_i}},\hspace{5mm}k=\overline{1,K}$$

**Почему именно экспонента?**

- $e^{\varphi_k}$ всегда положительна, независимо от знака $\varphi_k$
- Экспонента монотонна: больший $\varphi_k$ даёт больший $e^{\varphi_k}$
- Нормировка на сумму всех экспонент даёт значения от $0$ до $1$, сумма которых равна $1$

**Проверим для случая двух классов:**

Для $K = 2$ и $\varphi_1=\frac{\varphi}{2},\ \varphi_2=-\frac{\varphi}{2}$:

$$\begin{aligned}
&softmax\left(\frac{\varphi}{2},-\frac{\varphi}{2}\right)=\left(\frac{e^{\varphi/2}}{e^{\varphi/2}+e^{-\varphi/2}},\frac{e^{-\varphi/2}}{e^{\varphi/2}+e^{-\varphi/2}}\right) \\
&=\left(\frac{1}{1+e^{-\varphi}},\frac{e^{-\varphi}}{1+e^{-\varphi}}\right)=\left(sigmoid(\varphi),1-sigmoid(\varphi)\right)=(p,1-p)
\end{aligned}$$

@NB(`Softmax-функция — это обобщение логистической функции на случай $K > 2$.`)

Сравним Softmax с обычной функцией максимума (Hard max). В отличие от жёсткой функции Hard max, Softmax-функция гладкая и дифференцируемая, что позволяет использовать градиентные методы оптимизации.

|Hard max|Softmax|
|---|---|
|$(p_1,p_2)=\arg\max(\varphi_1,\varphi_2)$<br>$\varphi_1>\varphi_2\Rightarrow(1,0)$<br> $\varphi_1<\varphi_2\Rightarrow(0,1)$|$(p_1,p_2)=softmax(\varphi_1,\varphi_2)$<br>$\varphi_1>\varphi_2\Rightarrow(p_1,p_2),\ 0<p_2<p_1<1$<br>$\varphi_1<\varphi_2\Rightarrow(p_1,p_2),\ 0<p_1<p_2<1$|

Softmax возвращает $0$ и $1$ только в предельных случаях:

- Если $\varphi_1 \to +\infty, \varphi_2 \to -\infty$, то $p_1 \to 1, p_2 \to 0$
- Если $\varphi_1 \to -\infty, \varphi_2 \to +\infty$, то $p_1 \to 0, p_2 \to 1$

@img07(softmax.png, Softmax, 40)

Референсный класс
----

$$(p_1,...,p_K)=softmax(\varphi_1,...,\varphi_K),\hspace{5mm}\varphi_k=w_k^Tx,\ k=\overline{1,K}$$

Поскольку $\sum_{k=1}^Kp_k=1$, то последняя вероятность $p_K$ является зависимой переменной: $$p_K=1-\sum_{k=1}^{K-1}p_k$$

Это означает, что нет необходимости обучать параметры для $K$-го класса, т.е. $w_K$. Эти параметры являются **референсными**.

**Математическое обоснование:**

Добавим константный вектор $C$ ко всем векторам $w_1,...,w_K$:
$$\frac{e^{(w_k+C)^Tx}}{\sum\limits_{l=1}^Ke^{(w_l+C)^Tx}}=\frac{e^{w_k^Tx}e^{C^Tx}}{\sum\limits_{l=1}^Ke^{w_l^Tx}e^{C^Tx}}=\frac{e^{w_k^Tx}e^{C^Tx}}{e^{C^Tx}\sum\limits_{l=1}^Ke^{w_l^Tx}}=\frac{e^{w_k^Tx}}{\sum\limits_{l=1}^Ke^{w_l^Tx}}$$

Результат softmax остаётся неизменным! Это означает, что параметры определены с точностью до константы.

Пусть $C=-w_K$, тогда:
$$p_k=\frac{e^{(w_k+w_K)^Tx}}{\sum_{l=1}^Ke^{(w_l+w_K)^Tx}},\ k=\overline{1,K}$$

Определим: 
$$
w_1:=w_1-w_K
\\
...
\\
w_{K-1}:=w_{K-1}-w_K
\\
w_K:=0
$$

Нужно обучить только $K − 1$ вектор параметров.

**Апостериорные вероятности:**

$$p_1=\frac{e^{w_1^Tx}}{1+\sum\limits_{l=1}^{K-1}e^{w_l^Tx}},...,\ p_{K-1}=\frac{e^{w_{K-1}^Tx}}{1+\sum\limits_{l=1}^{K-1}e^{w_l^Tx}},\ p_K=\frac{1}{1+\sum\limits_{l=1}^{K-1}e^{w_l^Tx}}$$

**Преимущества использования референсного класса:**

- Уменьшение количества параметров на $M+1$ (размер одного вектора $w$)
- Ускорение обучения за счёт меньшего числа параметров
- Устранение избыточности в параметризации

----
#### Обучение логистической регрессии
----

Логистическая модель классов
----

Для обучения многоклассовой логистической регрессии построим статистическую модель, аналогично тому, как мы делали для бинарного случая.

$D=\{(x^{(1)},y^{(1)}),...,(x^{(n)},y^{(n)})\}$ -- обучающая выборка

**Статистическая модель:** Метка класса $y^{(i)}$ — это число от $1$ до $K$. Теперь мы не "подбрасываем монетку" (как в бинарном случае), а "бросаем игральный кубик" с $K$ гранями. Результат этого эксперимента — случайная величина с **мультиномиальным распределением**:
$$Y_i\sim Mult(1,p_1(x^{(i)},w),...,p_K(x^{(i)},w))$$
где $p_k(x^{(i)},w)=P(Y_i=k|x^{(i)},w)$ -- вероятность $k$-го класса для объекта $x^{(i)}$.

**Интерпретация:** Вероятности классов зависят от признаков объекта $x$ и параметров $w$. В одной области пространства признаков наиболее вероятен один класс, в другой области -- другой класс.

**One-hot кодирование меток:**

Для удобства работы с формулами переходим от числовых меток к векторному представлению:

$$y^{(i)} := (y_1^{(i)},...,y_K^{(i)}), \quad y_k^{(i)}=\begin{cases}1, & \text{если } y^{(i)}=k\\0, & \text{иначе}\end{cases}$$

Таким образом, $y^{(i)}$ — бинарный вектор длины $K$, содержащий одну единицу в позиции истинного класса и нули в остальных позициях.

**Примеры one-hot кодирования:**

- Класс $1$ из $3$: $(1,0,0)$
- Класс $2$ из $3$: $(0,1,0)$  
- Класс $3$ из $3$: $(0,0,1)$

**Вероятности в мультиномиальной модели:**
$$P(Y_i=k|x^{(i)},w)=p_k(x^{(i)},w)=\prod_{l=1}^K(p_l(x^{(i)},w))^{y_l^{(i)}}$$
В этом произведении все множители равны $1$, кроме одного: когда $l=k$, тогда $y_l^{(i)}=1$ и остаётся только $p_k(x^{(i)},w)$.

МП-оценивание параметров логистической регрессии
---

Для оценивания параметров $w$ может быть использован метод максимального правдоподобия (MLE).

**Функция правдоподобия:**
$$L(y^{(1)},...,y^{(n)},w)=\prod_{i=1}^n\prod_{l=1}^K(p_l(x^{(i)},w))^{y_l^{(i)}}\rightarrow\max_w$$

**Лог-функция правдоподобия:**
$$E(w)=-\sum_{i=1}^n\sum_{l=1}^Ky_l^{(i)}\ln p_l(x^{(i)},w)\rightarrow\min_w$$

**Логистическая модель для $p(x, w)$:**
$$(p_1(x,w),...,p_K(x,w))=softmax(w_1^Tx,...,w_{K-1}^Tx,0)$$
где последний аргумент равен $0$, поскольку мы используем референсный класс

**Матричная запись параметров:**

Все параметры можно записать в виде матрицы $w=(w_1^T,...,w_{K-1}^T)$ размера $(K-1) \times M$:
$$w = \begin{pmatrix}
w_{11} & w_{12} & \cdots & w_{1M} \\
w_{21} & w_{22} & \cdots & w_{2M} \\
\vdots & \vdots & \ddots & \vdots \\
w_{(K-1)1} & w_{(K-1)2} & \cdots & w_{(K-1)M}
\end{pmatrix}$$

где $M$ — количество признаков, $(K-1)$ — количество обучаемых классов (исключая референсный).

Градиентный спуск для многоклассовой задачи
---

Из-за нелинейности softmax-функции, для поиска минимума используется **градиентный спуск**:
$$w_k(t+1)=w_k(t)-\eta\frac{\partial E(t)}{\partial w_k},\hspace{5mm}w_k(0)=w_k^0,\hspace{5mm}k=\overline{1,K-1}$$
где $\frac{\partial E(t)}{\partial w_k}$ -- градиент, $\eta > 0$ -- шаг метода, $t$ -- номер итерации

**Производные softmax-функции** $(p_1,...,p_K)=softmax(\varphi_1,...,\varphi_K)$ (т.е. элементы матрицы Якоби) будут иметь следующий вид:
$$\frac{\partial p_l}{\partial \varphi_k}=p_l(\delta_{kl}-p_k)$$

где $\delta_{kl}=\begin{cases}1, & k=l\\0, & k \neq l\end{cases}$ — **символ Кронекера**.

Структура матрицы Якоби:

- На диагонали: $\frac{\partial p_k}{\partial \varphi_k} = p_k(1-p_k)$
- Вне диагонали: $\frac{\partial p_l}{\partial \varphi_k} = -p_l p_k$

**Производные лог-функции правдоподобия:**
$$\begin{aligned}
\frac{\partial E}{\partial w_{kj}}&=-\frac{\partial}{\partial w_{kj}}\sum\limits_{i=1}^n\sum\limits_{l=1}^Ky_l^{(i)}\ln p_l(x^{(i)},w) \\
&=-\sum\limits_{i=1}^n\sum\limits_{l=1}^Ky_l^{(i)}\frac{p_l(x^{(i)},w)(\delta_{kl}-p_k(x^{(i)},w))}{p_l(x^{(i)},w)}x_j^{(i)} \\
&=-\sum\limits_{i=1}^n\sum\limits_{l=1}^Ky_l^{(i)}(\delta_{kl}-p_k(x^{(i)},w))x_j^{(i)} \\
&=-\sum\limits_{i=1}^n\left(\sum\limits_{l=1}^Ky_l^{(i)}\delta_{kl}-p_k(x^{(i)},w)\sum\limits_{l=1}^Ky_l^{(i)}\right)x_j^{(i)} \\
&=-\sum\limits_{i=1}^n\left(y_k^{(i)}-p_k(x^{(i)},w)\right)x_j^{(i)}, \quad k=1,...,K-1,\ j=1,...,M
\end{aligned}$$

**Интерпретация градиента:** 
Для каждого обучающего примера $(x^{(i)}, y^{(i)})$ и каждого класса $k$:

- $p_k(x^{(i)},w)$ -- предсказанная вероятность класса $k$
- $y_k^{(i)}$ -- истинная метка ($0$ или $1$) для класса $k$  
- $(p_k(x^{(i)},w) - y_k^{(i)})$ -- ошибка предсказания для класса $k$

Градиент представляет собой сумму взвешенных ошибок по всем примерам и всем классам.

**Процедура обучения:**

1. Инициализация весов $w_k^{(0)}$ для всех классов $k=1,...,K-1$
2. Для каждой итерации $t$:

   - Вычислить вероятности $p_k(x^{(i)},w)$ для всех примеров и классов
   - Вычислить градиенты $\frac{\partial E}{\partial w_{kj}}$
   - Обновить веса: $w_{kj}^{(t+1)} = w_{kj}^{(t)} - \eta \frac{\partial E}{\partial w_{kj}}$

3. Повторять до сходимости

### Логистическая регрессия vs байесовская классификация

<!-- data-type="none"-->
||Байесовская классификация (генеративный подход)|Логистическая регрессия (дискриминативный подход)|
|---|---|---|
|**Основная задача**|Предполагает условные распределения признаков $p_X(x|y)$ и априорные распределения классов $p_Y(y)$, апостериорные вероятности вычисляются по теореме Байеса.|Предполагает модель апостериорных вероятностей классов $p_Y(y|x)$, настраиваются параметры модели.|
|**Преимущества**|1. Если выдвинутые предположения верны, то получаем оптимальный классификатор, любые усложнения приведут лишь к переобучению и моделированию шума<br>2. Простота и интерпретируемость: понятно, как принимается решение, можно посмотреть на оценённые распределения.|1. Работает без предположений о виде распределений.<br>2. Можно запустить процедуру оптимизации на любых данных и получить разумную конфигурацию весов, которые будут аппроксимировать дискриминантные функции лучше, чем если бы мы работали в неверных предположениях о нормальности.|
|**Недостатки**|1. Если предположения неверны (например, распределения не нормальные, неправильно оценили ширину окна в непараметрических методах), классификатор может быть далёк от оптимального.|1. Менее интерпретируемая, чем байесовская классификация.<br>2. Требует численных методов оптимизации|

Если данные действительно удовлетворяют предположениям наивного байесовского классификатора:

- Признаки независимы
- Распределены нормально  
- Ковариационная матрица одинакова для всех классов (shared)

То логистическая регрессия и байесовский классификатор дают **одинаковые результаты** (с некоторой погрешностью из-за численного метода)

- Байесовская классификация: веса находятся по аналитической формуле — мы сразу знаем решение.
- Логистическая регрессия: те же самые веса находятся через градиентный спуск (численно, с некоторой погрешностью)

----
### ❓ Вопросы для самопроверки
----


1. Что такое функция связи и зачем она нужна в логистической регрессии?

2. Объясните различия между логит-функцией и пробит-функцией связи. Какие у них преимущества и недостатки?

3. Как интерпретируются классификационные очки для логит-функции связи?

4. При каких условиях байесовский классификатор эквивалентен логистической регрессии?

5. Объясните статистическую модель для обучения логистической регрессии для случая бинарной и многоклассовой классификации.

6. Как связана задача обучения логистической регрессии с принципом минимизации эмпирического риска (ERM)?

7. Что такое классификационный отступ (margin) и как он интерпретируется?

8. Объясните, как работает градиентный спуск для обучения логистической регрессии. Почему нельзя решить задачу аналитически?

9. Что такое softmax-функция и как она обобщает логистическую функцию на многоклассовый случай?

10. Почему в многоклассовой логистической регрессии можно использовать референсный класс? Как это влияет на количество параметров?

11. В чем отличия дискриминативного подхода (логистическая регрессия) от генеративного подхода (байесовская классификация)?

12. Когда следует использовать логистическую регрессию, а когда — байесовские методы классификации? 

----
### ✅ Тест по разделу
----

@quizQuestion(`Что представляет собой функция связи в логистической регрессии?`)

<!-- data-randomize -->
[( )] Функция, которая связывает веса модели с признаками
[(x)] Функция, определяющая связь между вероятностями классов и классификационными очками
[( )] Функция активации в нейронной сети
[( )] Функция потерь для обучения модели

@quizQuestion(`Что показывает отношение шансов в логистической регрессии?`)

<!-- data-randomize -->
[(x)] Отношение вероятности положительного класса к вероятности отрицательного класса
[( )] Разность между вероятностями классов
[( )] Сумму вероятностей всех классов
[( )] Произведение весов модели на признаки

@quizQuestion(`В каком случае байесовская классификация и логистическая регрессия дают одинаковые результаты?`)

<!-- data-randomize -->
- [( )] Всегда
- [( )] Никогда
- [(X)] При нормальном наивном байесовском классификаторе с разделяемой ковариационной матрицей
- [( )] Только для бинарной классификации

@quizQuestion(`Что представляет собой margin в логистической регрессии?`)

<!-- data-randomize -->
[( )] Разность между предсказанной и истинной вероятностью
[(x)] Произведение истинной метки на классификационные очки
[( )] Сумму весов модели
[( )] Логарифм отношения шансов

@quizQuestion(`Какое распределение используется в статистической модели бинарной логистической регрессии?`)

<!-- data-randomize -->
[(x)] Распределение Бернулли
[( )] Нормальное распределение
[( )] Пуассоновское распределение
[( )] Равномерное распределение

@quizQuestion(`Почему в логистической регрессии используется градиентный спуск, а не аналитическое решение?`)

<!-- data-randomize -->
[( )] Аналитическое решение дает менее точные результаты
[(x)] Из-за нелинейности логистической функции аналитически решить задачу сложно
[( )] Градиентный спуск работает быстрее аналитических методов
[( )] Аналитическое решение не существует для этой задачи

@quizQuestion(`Что такое softmax-функция?`)

<!-- data-randomize -->
[( )] Функция для вычисления максимального значения в векторе
[(x)] Функция, обобщающая логистическую функцию на случай $K > 2$ классов
[( )] Функция активации для скрытых слоев нейронной сети
[( )] Функция для нормализации входных данных

@quizQuestion(`Сколько векторов параметров нужно обучить в многоклассовой логистической регрессии с $K$ классами при использовании референсного класса?`)

<!-- data-randomize -->
[( )] $K$ векторов
[(x)] $K-1$ вектор
[( )] $K+1$ вектор
[( )] $2K$ векторов

@quizQuestion(`Выберите преимущества логистической регрессии:`)

<!-- data-randomize -->
[[x]] Работает без предположений о виде распределений
[[x]] Выдает вероятности принадлежности к классам
[[ ]] Всегда дает более точные результаты, чем байесовские методы
[[x]] Может использоваться для многоклассовой классификации
[[ ]] Не требует итерационных методов обучения

@quizQuestion(`Что происходит с softmax-функцией при $K = 2$?`)

<!-- data-randomize -->
[( )] Она становится линейной функцией
[(x)] Она сводится к логистической функции
[( )] Она становится функцией распределения нормального распределения
[( )] Она не определена для $K = 2$

@quizQuestion(`В каком случае лучше использовать байесовскую классификацию вместо логистической регрессии?`)

<!-- data-randomize -->
[(x)] Когда данные действительно удовлетворяют предположениям о нормальности и независимости признаков
[( )] Когда данных очень много
[( )] Когда требуется высокая скорость обучения
[( )] Когда количество признаков превышает количество объектов